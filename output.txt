Folder Structure
--------------------------------------------------
./
    test_ground_truth_qa.py
    LICENSE
    pyproject.toml
    Dockerfile.backend
    .gitignore
    Dockerfile.nginx
    docker-compose.yml
    Dockerfile.database
    env.example
    Dockerfile.frontend
    .cursor/
    docker/
        db/
            init.sql
    frontend/
        app.py
    docs/
    logs/
        .gitkeep
    nginx/
        nginx.conf
    scripts/
        ingest_enhanced_data.py
        enhanced_hipaa_parser.py
        __init__.py
        cleanup.py
        pdf_extractor.py
        run_dev.sh
    .vscode/
    data/
        clean/
        raw/
    src/
        hipaa_qa/
            config.py
            __init__.py
            schemas.py
            main.py
            database/
                models.py
                __init__.py
                connection.py
                repository.py
            api/
                __init__.py
                main.py
                middleware.py
                dependencies.py
                state.py
                routes/
                    ingestion.py
                    health.py
                    __init__.py
                    qa.py
            services/
                enhanced_qa_service.py
                __init__.py
                embedding_service.py
                qa_service.py
                ingestion_service.py


File Contents
--------------------------------------------------


test_ground_truth_qa.py
File type: .py
#!/usr/bin/env python3
"""
Test script to validate HIPAA QA system with ground truth questions.
"""

import asyncio
import json
import sys
import time
from pathlib import Path
from typing import Dict, List, Tuple

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from hipaa_qa.config import Settings
from hipaa_qa.database import DatabaseManager, ChunkRepository
from hipaa_qa.services import EmbeddingService, QAService
from hipaa_qa.schemas import QuestionRequest


# Ground truth questions from test_questions_ground_truth.md
GROUND_TRUTH_QUESTIONS = [
    {
        "id": 1,
        "question": "What is the overall purpose of HIPAA Part 160?",
        "expected_key_points": [
            "Implements Social Security Act sections 1171-1180",
            "Administrative simplification provisions", 
            "General administrative requirements for HIPAA"
        ],
        "ground_truth": "The requirements of this subchapter implement sections 1171-1180 of the Social Security Act"
    },
    {
        "id": 2,
        "question": "Which part covers data privacy measures?",
        "expected_key_points": [
            "Part 164, Subpart E is the Privacy Rule",
            "Covers uses and disclosures of protected health information",
            "Includes sections 164.500-164.534"
        ],
        "ground_truth": "Part 164, Subpart E covers \"Privacy of Individually Identifiable Health Information\""
    },
    {
        "id": 3,
        "question": "What does \"minimum necessary\" mean in HIPAA terminology?",
        "expected_key_points": [
            "Limit PHI to minimum necessary for intended purpose",
            "Applies to uses, disclosures, and requests",
            "Requirement for reasonable efforts"
        ],
        "ground_truth": "make reasonable efforts to limit protected health information to the minimum necessary"
    },
    {
        "id": 4,
        "question": "Which entities are specifically regulated under HIPAA?",
        "expected_key_points": [
            "Health plans",
            "Health care clearinghouses", 
            "Health care providers (who transmit electronically)",
            "Business associates"
        ],
        "ground_truth": "health plan, health care clearinghouse, health care provider who transmits"
    },
    {
        "id": 5,
        "question": "What are the potential civil penalties for noncompliance?",
        "expected_key_points": [
            "Did not know: $100-$50,000 per violation",
            "Reasonable cause: $1,000-$50,000 per violation",
            "Willful neglect: $10,000-$50,000 per violation",
            "Up to $1.5M per year"
        ],
        "ground_truth": "Penalties range from $100 to $50,000 per violation"
    },
    {
        "id": 6,
        "question": "Does HIPAA mention encryption best practices?",
        "expected_key_points": [
            "Encryption is an \"addressable\" implementation specification",
            "Required for access control and transmission security",
            "Not mandatory but must be implemented if reasonable and appropriate"
        ],
        "ground_truth": "Implement a mechanism to encrypt and decrypt electronic protected health information"
    },
    {
        "id": 7,
        "question": "Can I disclose personal health information to family members?",
        "expected_key_points": [
            "Permitted under ¬ß 164.510",
            "Must be relevant to involvement in care",
            "Opportunity to agree/object required",
            "Special rules for deceased individuals"
        ],
        "ground_truth": "May disclose to family member, other relative, or close personal friend"
    },
    {
        "id": 8,
        "question": "If a covered entity outsources data processing, which sections apply?",
        "expected_key_points": [
            "Business associate relationship created",
            "Written business associate agreement required",
            "Business associate subject to HIPAA requirements",
            "Covered entity remains liable for oversight"
        ],
        "ground_truth": "Business associate provisions apply"
    },
    {
        "id": 9,
        "question": "Cite the specific regulation texts regarding permitted disclosures to law enforcement.",
        "expected_key_points": [
            "¬ß 164.512(f) is the main law enforcement section",
            "Six specific categories of permitted disclosures",
            "Each category has specific conditions and limitations",
            "Must meet applicable conditions"
        ],
        "ground_truth": "¬ß 164.512(f): Disclosures for law enforcement purposes are permitted"
    }
]


async def run_qa_tests():
    """Run QA tests with ground truth questions."""
    print("üöÄ Starting HIPAA QA System Ground Truth Tests")
    print("=" * 60)
    
    # Initialize services
    settings = Settings()
    # Override DB_HOST to connect to Docker database
    settings.db_host = "hipaa-qa-system-db-1"
    settings.db_user = "postgres"
    settings.db_password = "postgres"
    
    db_manager = DatabaseManager(settings)
    embedding_service = EmbeddingService(settings)
    repository = ChunkRepository(db_manager)
    qa_service = QAService(repository, embedding_service, settings)
    
    results = []
    total_questions = len(GROUND_TRUTH_QUESTIONS)
    
    print(f"üìä Testing {total_questions} ground truth questions...\n")
    
    for i, test_case in enumerate(GROUND_TRUTH_QUESTIONS, 1):
        print(f"üîç Question {i}/{total_questions}: {test_case['question'][:60]}...")
        
        start_time = time.time()
        try:
            # Ask the question
            response = await qa_service.answer_question(
                question=test_case["question"],
                max_chunks=5,
                similarity_threshold=0.6
            )
            
            processing_time = time.time() - start_time
            
            # Evaluate the response
            answer = response.answer.lower()
            ground_truth = test_case["ground_truth"].lower()
            
            # Simple relevance scoring
            key_words_found = sum(1 for key in test_case["expected_key_points"] 
                                if any(word.lower() in answer for word in key.split()))
            relevance_score = key_words_found / len(test_case["expected_key_points"])
            
            # Check if ground truth content is mentioned
            contains_ground_truth = any(word in answer for word in ground_truth.split() if len(word) > 3)
            
            result = {
                "question_id": test_case["id"],
                "question": test_case["question"],
                "answer": response.answer,
                "chunks_retrieved": response.chunks_retrieved,
                "confidence_score": response.confidence_score,
                "processing_time_ms": response.processing_time_ms,
                "relevance_score": relevance_score,
                "contains_ground_truth": contains_ground_truth,
                "sources": [{"section": src.section_id, "citation": src.cfr_citation} 
                           for src in response.sources],
                "evaluation": {
                    "relevant": relevance_score > 0.3,
                    "accurate": contains_ground_truth,
                    "comprehensive": response.chunks_retrieved >= 3
                }
            }
            
            results.append(result)
            
            # Print summary
            status = "‚úÖ" if result["evaluation"]["accurate"] else "‚ö†Ô∏è"
            print(f"   {status} Confidence: {response.confidence_score:.2f}, "
                  f"Chunks: {response.chunks_retrieved}, "
                  f"Time: {processing_time:.2f}s")
            print(f"   üìù Answer: {response.answer[:100]}...")
            print()
            
        except Exception as e:
            print(f"   ‚ùå Error: {e}")
            results.append({
                "question_id": test_case["id"],
                "question": test_case["question"],
                "error": str(e),
                "evaluation": {"relevant": False, "accurate": False, "comprehensive": False}
            })
            print()
    
    # Generate summary report
    await generate_test_report(results)
    
    # Cleanup
    await db_manager.close()
    
    return results


async def generate_test_report(results: List[Dict]):
    """Generate a comprehensive test report."""
    print("\n" + "=" * 60)
    print("üìä HIPAA QA SYSTEM TEST RESULTS")
    print("=" * 60)
    
    total_questions = len(results)
    successful_answers = sum(1 for r in results if not r.get("error"))
    accurate_answers = sum(1 for r in results if r.get("evaluation", {}).get("accurate", False))
    relevant_answers = sum(1 for r in results if r.get("evaluation", {}).get("relevant", False))
    
    print(f"üìà Overall Performance:")
    print(f"   ‚Ä¢ Total Questions: {total_questions}")
    print(f"   ‚Ä¢ Successful Responses: {successful_answers}/{total_questions} ({successful_answers/total_questions*100:.1f}%)")
    print(f"   ‚Ä¢ Accurate Answers: {accurate_answers}/{total_questions} ({accurate_answers/total_questions*100:.1f}%)")
    print(f"   ‚Ä¢ Relevant Answers: {relevant_answers}/{total_questions} ({relevant_answers/total_questions*100:.1f}%)")
    
    if successful_answers > 0:
        avg_confidence = sum(r.get("confidence_score", 0) for r in results if not r.get("error")) / successful_answers
        avg_chunks = sum(r.get("chunks_retrieved", 0) for r in results if not r.get("error")) / successful_answers
        avg_time = sum(r.get("processing_time_ms", 0) for r in results if not r.get("error")) / successful_answers
        
        print(f"\nüìä Performance Metrics:")
        print(f"   ‚Ä¢ Average Confidence: {avg_confidence:.2f}")
        print(f"   ‚Ä¢ Average Chunks Retrieved: {avg_chunks:.1f}")
        print(f"   ‚Ä¢ Average Response Time: {avg_time:.0f}ms")
    
    print(f"\nüìã Detailed Results:")
    for result in results:
        status = "‚úÖ" if result.get("evaluation", {}).get("accurate") else "‚ùå"
        print(f"   {status} Q{result['question_id']}: {result['question'][:50]}...")
        if result.get("error"):
            print(f"      ‚ùå Error: {result['error']}")
        else:
            eval_data = result.get("evaluation", {})
            print(f"      üìä Accurate: {'Yes' if eval_data.get('accurate') else 'No'}, "
                  f"Relevant: {'Yes' if eval_data.get('relevant') else 'No'}, "
                  f"Confidence: {result.get('confidence_score', 0):.2f}")
    
    # Save detailed results to file
    timestamp = int(time.time())
    results_file = f"qa_test_results_{timestamp}.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nüíæ Detailed results saved to: {results_file}")
    print("=" * 60)


async def main():
    """Main function."""
    try:
        await run_qa_tests()
        print("\nüéâ HIPAA QA System testing completed successfully!")
        return 0
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è Testing interrupted by user")
        return 1
    except Exception as e:
        print(f"\nüí• Fatal error during testing: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

--------------------------------------------------
File End
--------------------------------------------------


LICENSE
File type: no extension
MIT License

Copyright (c) 2025 HIPAA QA System

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

DISCLAIMER: This software is for informational purposes only and does not 
constitute legal advice. The HIPAA QA System is designed to assist with 
understanding HIPAA regulations but should not be relied upon as the sole 
source for compliance decisions. Always consult with qualified legal 
professionals for HIPAA compliance guidance and regulatory interpretation.

--------------------------------------------------
File End
--------------------------------------------------


pyproject.toml
File type: .toml
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "hipaa-qa-system"
version = "0.1.0"
description = "HIPAA Regulation QA System using RAG with OpenAI and pgvector"
readme = "README.md"
requires-python = ">=3.9"
license = {text = "MIT"}
authors = [
    {name = "HIPAA QA Team", email = "team@example.com"}
]
keywords = ["hipaa", "rag", "qa", "chatbot", "openai", "pgvector"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Legal",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]

dependencies = [
    "fastapi>=0.104.1",
    "uvicorn[standard]>=0.24.0",
    "pydantic>=2.5.0",
    "pydantic-settings>=2.1.0",
    "openai>=1.3.0",
    "asyncpg>=0.29.0",
    "pgvector>=0.2.4",
    "sqlalchemy[asyncio]>=2.0.23",
    "numpy>=1.24.0",
    "gradio>=4.7.1",
    "python-multipart>=0.0.6",
    "httpx>=0.25.0",
    "python-dotenv>=1.0.0",
    "tenacity>=8.2.3",
    "loguru>=0.7.2",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=1.7.0",
    "pre-commit>=3.5.0",
    "pytest-cov>=4.1.0",
]

[project.urls]
Homepage = "https://github.com/example/hipaa-qa-system"
Documentation = "https://github.com/example/hipaa-qa-system#readme"
Repository = "https://github.com/example/hipaa-qa-system.git"
Issues = "https://github.com/example/hipaa-qa-system/issues"

[tool.setuptools.packages.find]
where = ["src"]

[tool.black]
line-length = 88
target-version = ['py310']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88
known_first_party = ["hipaa_qa"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
check_untyped_defs = true
warn_redundant_casts = true
warn_unused_ignores = true
show_column_numbers = true
show_error_codes = true
show_error_context = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--verbose",
    "--cov=src/hipaa_qa",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
]
asyncio_mode = "auto"
filterwarnings = [
    "error",
    "ignore::UserWarning",
    "ignore::DeprecationWarning",
]

--------------------------------------------------
File End
--------------------------------------------------


Dockerfile.backend
File type: .backend
# Backend Dockerfile for HIPAA QA System
FROM python:3.11-slim

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONPATH=/app

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy dependency files and source code
COPY pyproject.toml ./
COPY src/ ./src/

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip setuptools wheel
RUN pip install --no-cache-dir -e .

# Create logs directory
RUN mkdir -p logs

# Copy data
COPY data/ ./data/

# Create non-root user for security
RUN groupadd -r hipaa && useradd -r -g hipaa hipaa
RUN chown -R hipaa:hipaa /app
USER hipaa

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health/live || exit 1

# Expose port
EXPOSE 8000

# Command to run the application
CMD ["python", "-m", "src.hipaa_qa.main"]

--------------------------------------------------
File End
--------------------------------------------------


.gitignore
File type: no extension
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# PEP 582
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# Docker
.dockerignore

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs
logs/
*.log

# Data files (keep structure but not actual data)
data/raw/*.pdf
data/clean/*.json
data/processed/

# Test results
test_results/
*_test_results.json

# Temporary files
tmp/
temp/
*.tmp

# Environment specific
.env.local
.env.development
.env.test
.env.production

# Docker volumes
postgres_data/

# Cloudflare
.cloudflared/

# Editor specific
.cursor/
*.code-workspace

# Application specific logs
enhanced_*.log
section_parser.log
pdf_extraction.log

# Test outputs
enhanced_test_results.json

# Backup files
*.bak
*.backup

--------------------------------------------------
File End
--------------------------------------------------


Dockerfile.nginx
File type: .nginx
FROM nginx:alpine

# Copy nginx configuration
COPY nginx/nginx.conf /etc/nginx/nginx.conf

# Create log directory
RUN mkdir -p /var/log/nginx

# Expose port 80
EXPOSE 80

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost/health || exit 1

# Install curl for health check
RUN apk add --no-cache curl

CMD ["nginx", "-g", "daemon off;"]

--------------------------------------------------
File End
--------------------------------------------------


docker-compose.yml
File type: .yml
version: '3.8'

services:
  # PostgreSQL database with pgvector
  db:
    build:
      context: .
      dockerfile: Dockerfile.database
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-hipaa_qa}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/db/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    ports:
      - "${DB_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-hipaa_qa}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    networks:
      - hipaa_network

  # Backend FastAPI service
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    environment:
      # OpenAI Configuration
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_EMBEDDING_MODEL: ${OPENAI_EMBEDDING_MODEL:-text-embedding-3-large}
      OPENAI_CHAT_MODEL: ${OPENAI_CHAT_MODEL:-gpt-4}
      
      # Database Configuration
      DB_HOST: db
      DB_PORT: 5432
      POSTGRES_DB: ${POSTGRES_DB:-hipaa_qa}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      
      # API Configuration
      API_HOST: 0.0.0.0
      API_PORT: 8000
      API_DEBUG: ${DEBUG:-false}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      LOG_FORMAT: ${LOG_FORMAT:-json}
      
      # Vector Search Configuration
      EMBEDDING_DIMENSION: 3072
      SIMILARITY_THRESHOLD: 0.4
      MAX_CHUNKS_RETRIEVED: 5
      
      # Data Configuration
      CHUNKS_FILE: /app/data/clean/hipaa_rag_chunks.json
    # Remove external port exposure - access through nginx
    expose:
      - "8000"
    depends_on:
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - hipaa_network
    volumes:
      - ./logs:/app/logs
      
  # Frontend Gradio service
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    environment:
      BACKEND_URL: http://backend:8000
      GRADIO_HOST: 0.0.0.0
      GRADIO_PORT: 7860
      GRADIO_SHARE: ${GRADIO_SHARE:-false}
    # Remove external port exposure - access through nginx
    expose:
      - "7860"
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - hipaa_network

  # Nginx reverse proxy
  nginx:
    build:
      context: .
      dockerfile: Dockerfile.nginx
    ports:
      - "80:80"
      - "${FRONTEND_PORT:-7860}:80"  # Map frontend port to nginx
    depends_on:
      frontend:
        condition: service_healthy
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    networks:
      - hipaa_network
    volumes:
      - ./logs:/var/log/nginx

  # Cloudflare Tunnel for public access
  cloudflared:
    image: cloudflare/cloudflared:latest
    command: tunnel --no-autoupdate --url http://nginx:80
    depends_on:
      nginx:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - hipaa_network
    environment:
      - TUNNEL_METRICS=0.0.0.0:9126
    labels:
      - "com.centurylinklabs.watchtower.enable=true"

volumes:
  postgres_data:
    driver: local

networks:
  hipaa_network:
    driver: bridge

--------------------------------------------------
File End
--------------------------------------------------


Dockerfile.database
File type: .database
# Database Dockerfile with pgvector extension
FROM pgvector/pgvector:pg15

# Copy initialization scripts
COPY docker/db/init.sql /docker-entrypoint-initdb.d/

# pgvector is already configured in the base image

--------------------------------------------------
File End
--------------------------------------------------


env.example
File type: .example
# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
MISTRAL_API_KEY=your_mistral_api_key_here

# OpenAI Configuration (Required)
OPENAI_EMBEDDING_MODEL=text-embedding-3-large
OPENAI_CHAT_MODEL=gpt-4-turbo

# Database Configuration
POSTGRES_DB=hipaa_qa
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
DB_PORT=5432

# API Configuration
API_PORT=8000
FRONTEND_PORT=7860
DEBUG=false
LOG_LEVEL=INFO
LOG_FORMAT=json

# Gradio Configuration
GRADIO_SHARE=false

# Vector Search Configuration
EMBEDDING_DIMENSION=3072
SIMILARITY_THRESHOLD=0.4
MAX_CHUNKS_RETRIEVED=5

# Performance Configuration
BATCH_SIZE=100
DB_POOL_SIZE=10
API_WORKERS=1
DB_HOST=localhost


--------------------------------------------------
File End
--------------------------------------------------


Dockerfile.frontend
File type: .frontend
# Frontend Dockerfile for HIPAA QA System
FROM python:3.11-slim

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
RUN pip install --no-cache-dir --upgrade pip
RUN pip install --no-cache-dir \
    gradio>=4.7.1 \
    httpx>=0.25.0 \
    loguru>=0.7.2

# Copy frontend code
COPY frontend/ ./

# Create non-root user for security
RUN groupadd -r gradio && useradd -r -g gradio gradio
RUN chown -R gradio:gradio /app
USER gradio

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:7860 || exit 1

# Expose port
EXPOSE 7860

# Command to run the application
CMD ["python", "app.py"]

--------------------------------------------------
File End
--------------------------------------------------


docker/db/init.sql
File type: .sql
-- Database initialization script for HIPAA QA System

-- Create vector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Verify extension installation
SELECT extname, extversion FROM pg_extension WHERE extname = 'vector';

-- Create database user if not exists (optional, as postgres user is default)
-- You can customize this based on your needs

-- Log successful initialization
\echo 'pgvector extension installed successfully'

--------------------------------------------------
File End
--------------------------------------------------


frontend/app.py
File type: .py
"""Gradio frontend for HIPAA QA System."""

import asyncio
import json
import os
import sys
import time
from typing import Dict, List, Optional, Tuple

import gradio as gr
import httpx
from loguru import logger

# Configure logging
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> | <level>{message}</level>",
    level="INFO"
)

# Configuration
BACKEND_URL = os.getenv("BACKEND_URL", "http://backend:8000")
GRADIO_HOST = os.getenv("GRADIO_HOST", "0.0.0.0")
GRADIO_PORT = int(os.getenv("GRADIO_PORT", "7860"))
GRADIO_SHARE = os.getenv("GRADIO_SHARE", "false").lower() == "true"

# HTTP client with timeout
http_client = httpx.AsyncClient(timeout=60.0)


async def ask_question(
    question: str,
    max_chunks: int = 5,
    similarity_threshold: float = 0.4,
    include_sources: bool = True,
) -> Tuple[str, str]:
    """
    Ask a question to the HIPAA QA backend.
    
    Args:
        question: The user's question
        max_chunks: Maximum number of context chunks to retrieve
        similarity_threshold: Minimum similarity threshold
        include_sources: Whether to include source information
        
    Returns:
        Tuple of (answer_text, sources_text)
    """
    if not question.strip():
        return "Please enter a question.", ""
        
    logger.info(f"Processing question: {question[:100]}...")
    
    try:
        # Prepare request
        request_data = {
            "question": question.strip(),
            "max_chunks": max_chunks,
            "similarity_threshold": similarity_threshold,
            "include_metadata": include_sources,
        }
        
        # Make API request
        start_time = time.time()
        response = await http_client.post(
            f"{BACKEND_URL}/qa/ask",
            json=request_data,
            headers={"Content-Type": "application/json"}
        )
        
        response.raise_for_status()
        result = response.json()
        
        processing_time = time.time() - start_time
        
        # Extract answer
        answer = result.get("answer", "No answer provided")
        
        # Format sources if requested
        sources_text = ""
        if include_sources and result.get("sources"):
            sources_text = format_sources(result["sources"])
            
        # Add metadata footer
        metadata = format_metadata(result, processing_time)
        answer_with_metadata = f"{answer}\n\n{metadata}"
        
        logger.info(f"Question processed successfully in {processing_time:.2f}s")
        return answer_with_metadata, sources_text
        
    except httpx.RequestError as e:
        error_msg = f"Connection error: {str(e)}"
        logger.error(error_msg)
        return f"‚ùå {error_msg}", ""
        
    except httpx.HTTPStatusError as e:
        try:
            error_detail = e.response.json().get("detail", str(e))
        except:
            error_detail = str(e)
        error_msg = f"API error ({e.response.status_code}): {error_detail}"
        logger.error(error_msg)
        return f"‚ùå {error_msg}", ""
        
    except Exception as e:
        error_msg = f"Unexpected error: {str(e)}"
        logger.error(error_msg)
        return f"‚ùå {error_msg}", ""


def format_sources(sources: List[Dict]) -> str:
    """Format source references for display."""
    if not sources:
        return "No sources found."
        
    formatted_sources = ["## üìö Source References\n"]
    
    for i, source in enumerate(sources, 1):
        cfr_citation = source.get("cfr_citation", "Unknown citation")
        section_title = source.get("section_title", "Unknown section")
        similarity_score = source.get("similarity_score", 0.0)
        content_excerpt = source.get("content_excerpt", "")
        
        # Truncate long excerpts
        if len(content_excerpt) > 300:
            content_excerpt = content_excerpt[:300] + "..."
            
        source_text = (
            f"**{i}. {cfr_citation}** - {section_title}\n"
            f"*Similarity: {similarity_score:.3f}*\n\n"
            f"{content_excerpt}\n\n"
            "---\n"
        )
        formatted_sources.append(source_text)
        
    return "".join(formatted_sources)


def format_metadata(result: Dict, processing_time: float) -> str:
    """Format metadata information."""
    chunks_retrieved = result.get("chunks_retrieved", 0)
    confidence_score = result.get("confidence_score")
    model_used = result.get("model_used", "Unknown")
    backend_time = result.get("processing_time_ms", 0)
    
    metadata_parts = [
        "---",
        "### ‚ÑπÔ∏è Response Details",
        f"‚Ä¢ **Chunks Retrieved:** {chunks_retrieved}",
        f"‚Ä¢ **Model:** {model_used}",
        f"‚Ä¢ **Processing Time:** {backend_time}ms (backend) + {processing_time*1000:.0f}ms (total)",
    ]
    
    if confidence_score is not None:
        metadata_parts.append(f"‚Ä¢ **Confidence:** {confidence_score:.3f}")
        
    return "\n".join(metadata_parts)


def get_backend_health() -> str:
    """Check backend health status."""
    try:
        # Use synchronous httpx client for Gradio compatibility
        with httpx.Client(timeout=10.0) as sync_client:
            response = sync_client.get(f"{BACKEND_URL}/health/")
            response.raise_for_status()
            health_data = response.json()
            
            status = health_data.get("status", "unknown")
            chunks_indexed = health_data.get("chunks_indexed", 0)
            
            if status == "healthy":
                return f"‚úÖ Backend healthy ({chunks_indexed:,} chunks indexed)"
            elif status == "degraded":
                return f"‚ö†Ô∏è Backend degraded ({chunks_indexed:,} chunks indexed)"
            else:
                return f"‚ùå Backend unhealthy"
                
    except Exception as e:
        return f"‚ùå Backend unreachable: {str(e)}"


async def get_example_questions() -> List[str]:
    """Get example questions to populate the interface."""
    return [
        "What is a business associate under HIPAA?",
        "What are the requirements for authorization under the Privacy Rule?",
        "What constitutes a breach of protected health information?",
        "What are the administrative safeguards required by the Security Rule?",
        "Who must comply with HIPAA regulations?",
        "What is the definition of protected health information?",
        "What are the penalties for HIPAA violations?",
        "What are the minimum necessary standards?",
        "How should covered entities handle patient access requests?",
        "What encryption standards are required under HIPAA?",
    ]


def create_interface() -> gr.Interface:
    """Create the Gradio interface."""
    
    # Custom CSS for better appearance
    custom_css = """
    .gradio-container {
        max-width: 1200px !important;
    }
    .answer-box {
        background-color: #f8f9fa;
        border-left: 4px solid #007bff;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    .sources-box {
        background-color: #f1f3f4;
        border-left: 4px solid #28a745;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    """
    
    # Interface function that handles both sync and async
    def interface_fn(question, max_chunks, similarity_threshold, include_sources):
        import asyncio
        
        # Create new event loop if needed
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
        # Run the async function
        if loop.is_running():
            # If already in an async context, create a new thread
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    lambda: asyncio.run(
                        ask_question(question, max_chunks, similarity_threshold, include_sources)
                    )
                )
                return future.result()
        else:
            return loop.run_until_complete(
                ask_question(question, max_chunks, similarity_threshold, include_sources)
            )
    
    # Create interface
    with gr.Blocks(
        title="HIPAA QA System",
        theme=gr.themes.Soft(),
        css=custom_css
    ) as interface:
        
        # Header
        gr.Markdown(
            """
            # üè• HIPAA QA System
            ## Ask questions about HIPAA regulations and get precise answers with citations
            
            This system uses **Retrieval-Augmented Generation (RAG)** to answer questions about HIPAA regulations 
            (45 CFR Parts 160, 162, and 164) with exact citations from the law.
            """
        )
        
        # Health status
        health_status = gr.Markdown("üîÑ Checking backend status...")
        
        with gr.Row():
            with gr.Column(scale=2):
                # Question input
                question_input = gr.Textbox(
                    label="Your Question",
                    placeholder="e.g., What is a business associate under HIPAA?",
                    lines=3,
                    max_lines=5,
                )
                
                # Example questions
                example_questions = gr.Dropdown(
                    label="Example Questions",
                    choices=[
                        "What is a business associate under HIPAA?",
                        "What are the requirements for authorization under the Privacy Rule?",
                        "What constitutes a breach of protected health information?",
                        "What are the administrative safeguards required by the Security Rule?",
                        "Who must comply with HIPAA regulations?",
                        "What is the definition of protected health information?",
                        "What are the penalties for HIPAA violations?",
                        "What are the minimum necessary standards?",
                        "How should covered entities handle patient access requests?",
                        "What encryption standards are required under HIPAA?",
                    ],
                    interactive=True,
                )
                
                # When example is selected, populate the question input
                example_questions.change(
                    fn=lambda x: x,
                    inputs=example_questions,
                    outputs=question_input,
                )
                
            with gr.Column(scale=1):
                # Advanced settings
                with gr.Accordion("Advanced Settings", open=False):
                    max_chunks = gr.Slider(
                        minimum=1,
                        maximum=10,
                        value=5,
                        step=1,
                        label="Max Context Chunks",
                        info="Maximum number of regulation sections to retrieve"
                    )
                    
                    similarity_threshold = gr.Slider(
                        minimum=0.0,
                        maximum=1.0,
                        value=0.4,
                        step=0.05,
                        label="Similarity Threshold",
                        info="Minimum similarity score for relevant chunks"
                    )
                    
                    include_sources = gr.Checkbox(
                        value=True,
                        label="Show Sources",
                        info="Display source references and citations"
                    )
        
        # Submit button
        submit_btn = gr.Button("Ask Question", variant="primary", size="lg")
        
        # Output areas
        with gr.Row():
            with gr.Column(scale=2):
                answer_output = gr.Markdown(
                    label="Answer",
                    elem_classes=["answer-box"],
                )
                
            with gr.Column(scale=1):
                sources_output = gr.Markdown(
                    label="Sources",
                    elem_classes=["sources-box"],
                    visible=True,
                )
        
        # Event handlers
        submit_btn.click(
            fn=interface_fn,
            inputs=[question_input, max_chunks, similarity_threshold, include_sources],
            outputs=[answer_output, sources_output],
            show_progress=True,
        )
        
        # Also allow Enter key in question input
        question_input.submit(
            fn=interface_fn,
            inputs=[question_input, max_chunks, similarity_threshold, include_sources],
            outputs=[answer_output, sources_output],
            show_progress=True,
        )
        
        # Show/hide sources based on checkbox
        include_sources.change(
            fn=lambda x: gr.update(visible=x),
            inputs=include_sources,
            outputs=sources_output,
        )
        
        # Footer
        gr.Markdown(
            """
            ---
            **About this system:** This HIPAA QA bot uses OpenAI's GPT-4 and text-embedding-3-large models 
            with a PostgreSQL vector database to provide accurate answers with exact citations from the 
            HIPAA regulations. All responses include specific CFR section references.
            
            **Disclaimer:** This system is for informational purposes only and does not constitute legal advice.
            """
        )
        
        # Update health status on load
        interface.load(
            fn=get_backend_health,
            outputs=health_status,
        )
    
    return interface


def main():
    """Main function to run the Gradio app."""
    logger.info("Starting HIPAA QA Frontend...")
    logger.info(f"Backend URL: {BACKEND_URL}")
    logger.info(f"Host: {GRADIO_HOST}, Port: {GRADIO_PORT}")
    
    # Create and launch interface
    interface = create_interface()
    
    interface.launch(
        server_name=GRADIO_HOST,
        server_port=GRADIO_PORT,
        share=GRADIO_SHARE,
        show_error=True,
    )


if __name__ == "__main__":
    main()

--------------------------------------------------
File End
--------------------------------------------------


logs/.gitkeep
File type: no extension


--------------------------------------------------
File End
--------------------------------------------------


nginx/nginx.conf
File type: .conf
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }
    
    upstream frontend {
        server frontend:7860;
    }
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=frontend:10m rate=30r/s;
    
    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Referrer-Policy "strict-origin-when-cross-origin";
    
    # MIME types
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
    
    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;
    
    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;
    
    server {
        listen 80;
        server_name localhost;
        
        # Frontend routes
        location / {
            limit_req zone=frontend burst=50 nodelay;
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocket support for Gradio
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            
            # Timeouts for long-running operations
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 300s;
        }
        
        # API routes
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://backend/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # API timeouts
            proxy_connect_timeout 30s;
            proxy_send_timeout 60s;
            proxy_read_timeout 120s;
        }
        
        # Health check endpoint
        location /health {
            access_log off;
            return 200 "nginx healthy\n";
            add_header Content-Type text/plain;
        }
        
        # Security: block access to sensitive files
        location ~ /\. {
            deny all;
            access_log off;
            log_not_found off;
        }
        
        location ~ \.(env|log|conf)$ {
            deny all;
            access_log off;
            log_not_found off;
        }
    }
}

--------------------------------------------------
File End
--------------------------------------------------


scripts/ingest_enhanced_data.py
File type: .py
#!/usr/bin/env python3
"""
Enhanced Data Ingestion Script

This script ingests the enhanced HIPAA chunks with improved metadata
into the PostgreSQL database with pgvector for semantic search.
"""

import asyncio
import json
import logging
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from hipaa_qa.config import Settings
from hipaa_qa.database import DatabaseManager, ChunkRepository, wait_for_database
from hipaa_qa.services import EmbeddingService
from hipaa_qa.schemas import DocumentChunk, ChunkMetadata, SectionType, ContentType, ComplianceLevel

# Enhanced logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_ingestion.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


async def ingest_enhanced_chunks():
    """Ingest enhanced HIPAA chunks into the database."""
    logger.info("üöÄ Starting enhanced data ingestion...")
    
    # Load settings
    settings = Settings()
    
    # Wait for database
    logger.info("‚è≥ Waiting for database connection...")
    db_available = await wait_for_database(max_retries=30, retry_interval=2.0, settings=settings)
    if not db_available:
        raise RuntimeError("Database connection failed")
    
    # Initialize services
    db_manager = DatabaseManager(settings)
    embedding_service = EmbeddingService(settings)
    
    try:
        # Load enhanced chunks
        chunks_file = Path("data/clean/enhanced_hipaa_chunks.json")
        if not chunks_file.exists():
            raise FileNotFoundError(f"Enhanced chunks file not found: {chunks_file}")
        
        logger.info(f"üìñ Loading enhanced chunks from {chunks_file}")
        with open(chunks_file, 'r', encoding='utf-8') as f:
            enhanced_data = json.load(f)
        
        logger.info(f"üìã Loaded {len(enhanced_data)} enhanced chunks")
        
        # Clear existing data
        repository = ChunkRepository(db_manager)
        logger.info("üßπ Clearing existing chunks...")
        deleted_count = await repository.delete_all_chunks()
        logger.info(f"üóëÔ∏è Deleted {deleted_count} existing chunks")
        
        # Process chunks in batches
        batch_size = 50
        total_batches = (len(enhanced_data) + batch_size - 1) // batch_size
        
        all_chunks = []
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(enhanced_data))
            batch_data = enhanced_data[start_idx:end_idx]
            
            logger.info(f"üîÑ Processing batch {batch_idx + 1}/{total_batches} (chunks {start_idx}-{end_idx-1})")
            
            # Generate embeddings for batch
            batch_texts = [chunk['content'] for chunk in batch_data]
            batch_embeddings = await embedding_service.embed_texts_batch(batch_texts)
            
            # Create DocumentChunk objects
            batch_chunks = []
            for i, (chunk_data, embedding) in enumerate(zip(batch_data, batch_embeddings)):
                metadata_dict = chunk_data['metadata']
                
                # Map section type
                section_type_map = {
                    'part': SectionType.PART,
                    'subpart': SectionType.SUBPART,
                    'section': SectionType.SECTION,
                    'subsection': SectionType.SUBSECTION,
                    'paragraph': SectionType.PARAGRAPH
                }
                section_type = section_type_map.get(metadata_dict.get('section_type', 'section'), SectionType.SECTION)
                
                # Enhanced content categorization
                content_categories = metadata_dict.get('content_categories', [])
                if 'definitions' in content_categories:
                    content_type = ContentType.DEFINITION
                elif 'requirements' in content_categories:
                    content_type = ContentType.REQUIREMENT
                elif 'penalties' in content_categories:
                    content_type = ContentType.PENALTY
                else:
                    content_type = ContentType.GENERAL
                
                # Build references and key terms from metadata
                references = []
                if metadata_dict.get('cfr_citation'):
                    references.append(metadata_dict['cfr_citation'])
                if metadata_dict.get('section_id'):
                    references.append(f"¬ß {metadata_dict['section_id']}")
                
                key_terms = list(metadata_dict.get('key_concepts', []))
                
                # Create proper ChunkMetadata
                chunk_metadata = ChunkMetadata(
                    section_id=metadata_dict['section_id'],
                    section_type=section_type,
                    section_title=metadata_dict['section_title'],
                    full_reference=metadata_dict.get('full_reference', f"45 CFR ¬ß {metadata_dict['section_id']}"),
                    cfr_citation=metadata_dict.get('cfr_citation'),
                    parent_section=metadata_dict.get('parent_section'),
                    hierarchy_level=metadata_dict.get('hierarchy_level', 3),
                    chunk_index=metadata_dict.get('chunk_index', 0),
                    total_chunks=metadata_dict.get('total_chunks', 1),
                    chunk_size=len(chunk_data['content']),
                    word_count=metadata_dict.get('word_count', len(chunk_data['content'].split())),
                    contains_definitions='definitions' in content_categories,
                    contains_penalties='penalties' in content_categories,
                    contains_requirements='requirements' in content_categories,
                    references=references,
                    key_terms=key_terms,
                    content_type=content_type,
                    compliance_level=ComplianceLevel.INFORMATIONAL
                )
                
                chunk = DocumentChunk(
                    chunk_id=chunk_data['chunk_id'],
                    content=chunk_data['content'],
                    metadata=chunk_metadata,
                    embedding=embedding
                )
                batch_chunks.append(chunk)
            
            all_chunks.extend(batch_chunks)
            
            # Insert batch into database
            try:
                await repository.bulk_insert_chunks(batch_chunks)
                logger.info(f"‚úÖ Successfully inserted batch {batch_idx + 1}")
            except Exception as e:
                logger.error(f"‚ùå Failed to insert batch {batch_idx + 1}: {e}")
                raise
        
        # Verify insertion
        total_inserted = await repository.count_chunks()
        logger.info(f"üìä Verification: {total_inserted} chunks in database")
        
        if total_inserted != len(enhanced_data):
            logger.warning(f"‚ö†Ô∏è Mismatch: Expected {len(enhanced_data)}, got {total_inserted}")
        else:
            logger.info("‚úÖ All enhanced chunks successfully ingested!")
        
        # Test retrieval with a sample query
        logger.info("üîç Testing enhanced retrieval...")
        test_embedding = await embedding_service.embed_text("What is protected health information?")
        test_results = await repository.similarity_search(
            query_embedding=test_embedding,
            limit=3,
            similarity_threshold=0.1
        )
        
        logger.info(f"üéØ Test retrieval returned {len(test_results)} results")
        for i, result in enumerate(test_results):
            chunk_id, content, section_id, cfr_citation, references_json, key_terms_json, similarity = result
            logger.info(f"  {i+1}. {cfr_citation or section_id} (similarity: {similarity:.3f})")
        
    except Exception as e:
        logger.error(f"‚ùå Enhanced ingestion failed: {e}")
        raise
    finally:
        await db_manager.close()
    
    logger.info("üéâ Enhanced data ingestion completed successfully!")


async def main():
    """Main function."""
    try:
        await ingest_enhanced_chunks()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Ingestion interrupted by user")
    except Exception as e:
        logger.error(f"üí• Fatal error: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

--------------------------------------------------
File End
--------------------------------------------------


scripts/enhanced_hipaa_parser.py
File type: .py
"""
Enhanced HIPAA Regulations Parser for Maximum RAG Accuracy

This parser addresses specific issues identified in testing:
1. Better distinction between Privacy Rule (164.E) and Security Rule (164.C)
2. Improved semantic chunking for precise retrieval
3. Enhanced metadata for exact citation matching
4. Context-aware section splitting

Target: 100% accuracy on evaluation questions
"""

import re
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Set
from dataclasses import dataclass, asdict
from datetime import datetime
import argparse

# Enhanced logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhanced_parser.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class EnhancedSectionMetadata:
    """Enhanced metadata for precise RAG retrieval."""
    section_id: str
    section_type: str  
    section_title: str
    full_reference: str
    cfr_citation: str
    parent_section: Optional[str]
    hierarchy_level: int
    
    # Enhanced categorization
    regulation_domain: str  # 'privacy', 'security', 'transactions', 'general', 'penalties'
    content_categories: Set[str]  # 'definitions', 'requirements', 'penalties', 'exceptions'
    key_concepts: Set[str]  # Primary concepts for semantic matching
    
    # Structural information
    word_count: int
    character_count: int
    chunk_index: int
    total_chunks: int
    
    # Citation helpers
    exact_quote_ranges: List[Tuple[int, int]]  # Character ranges for exact quotes
    subsection_map: Dict[str, str]  # subsection -> description


@dataclass
class EnhancedChunk:
    """Enhanced chunk with comprehensive metadata."""
    chunk_id: int
    content: str
    metadata: EnhancedSectionMetadata
    embedding: Optional[List[float]] = None


class EnhancedHIPAAParser:
    """
    Enhanced parser targeting 100% accuracy on HIPAA QA tasks.
    
    Key improvements:
    - Precise Privacy vs Security rule distinction
    - Context-aware semantic chunking
    - Enhanced metadata for exact matching
    - Better cross-reference handling
    """
    
    def __init__(self):
        self.section_patterns = self._compile_enhanced_patterns()
        self.domain_classifiers = self._compile_domain_classifiers()
        self.concept_extractors = self._compile_concept_extractors()
        
        # Enhanced tracking
        self.parsing_stats = {
            'total_chunks': 0,
            'privacy_chunks': 0,
            'security_chunks': 0,
            'definition_chunks': 0,
            'penalty_chunks': 0,
            'requirement_chunks': 0
        }
    
    def _compile_enhanced_patterns(self) -> Dict[str, re.Pattern]:
        """Enhanced patterns for precise section identification."""
        return {
            # Core structure patterns
            'part': re.compile(r'^PART\s+(\d+)[‚Äî\-]\s*(.+?)(?:\s+\.{3,}\s*\d+)?$', re.IGNORECASE | re.MULTILINE),
            'subpart': re.compile(r'^SUBPART\s+([A-Z])[‚Äî\-]\s*(.+?)(?:\s+\.{3,}\s*\d+)?$', re.IGNORECASE | re.MULTILINE),
            'section': re.compile(r'^[¬ß¬ß]\s*(\d+)\.(\d+)\s+(.+?)(?:\s+\.{3,}\s*\d+)?$', re.MULTILINE),
            'subsection_header': re.compile(r'^##\s+[¬ß¬ß]\s*(\d+)\.(\d+)\s+(.+)$', re.MULTILINE),
            
            # Enhanced subsection patterns
            'numbered_subsection': re.compile(r'^\((\d+)\)\s+(.+)', re.MULTILINE),
            'lettered_subsection': re.compile(r'^\(([a-z])\)\s+(.+)', re.MULTILINE),
            'roman_subsection': re.compile(r'^\(([ivxlc]+)\)\s+(.+)', re.MULTILINE | re.IGNORECASE),
            
            # Privacy Rule specific patterns
            'privacy_subpart': re.compile(r'SUBPART\s+E[‚Äî\-]\s*PRIVACY\s+OF\s+INDIVIDUALLY\s+IDENTIFIABLE\s+HEALTH\s+INFORMATION', re.IGNORECASE),
            'privacy_section': re.compile(r'[¬ß¬ß]\s*164\.5\d+', re.IGNORECASE),
            
            # Security Rule specific patterns  
            'security_subpart': re.compile(r'SUBPART\s+C[‚Äî\-]\s*SECURITY\s+STANDARDS', re.IGNORECASE),
            'security_section': re.compile(r'[¬ß¬ß]\s*164\.3\d+', re.IGNORECASE),
            
            # Citation patterns
            'cfr_full': re.compile(r'45\s+CFR\s+[¬ß¬ß]?\s*(\d+)\.(\d+)', re.IGNORECASE),
            'section_ref': re.compile(r'[¬ß¬ß]\s*(\d+)\.(\d+)(?:\([a-z0-9]+\))*', re.IGNORECASE),
        }
    
    def _compile_domain_classifiers(self) -> Dict[str, Dict[str, re.Pattern]]:
        """Compile patterns to classify regulation domains."""
        return {
            'privacy': {
                'keywords': re.compile(r'\b(?:privacy|protected health information|phi|uses?\s+and\s+disclosures?|authorization|minimum\s+necessary)\b', re.IGNORECASE),
                'sections': re.compile(r'164\.5\d+'),
                'subpart': re.compile(r'subpart\s+e', re.IGNORECASE)
            },
            'security': {
                'keywords': re.compile(r'\b(?:security|safeguards|access\s+control|encryption|integrity|transmission|audit)\b', re.IGNORECASE),
                'sections': re.compile(r'164\.3\d+'),
                'subpart': re.compile(r'subpart\s+c', re.IGNORECASE)
            },
            'penalties': {
                'keywords': re.compile(r'\b(?:civil\s+money\s+penalty|violation|fine|penalty|sanctions?)\b', re.IGNORECASE),
                'sections': re.compile(r'160\.4\d+')
            },
            'general': {
                'keywords': re.compile(r'\b(?:definitions?|applicability|compliance|general\s+provisions)\b', re.IGNORECASE),
                'sections': re.compile(r'160\.1\d+')
            },
            'transactions': {
                'keywords': re.compile(r'\b(?:transactions?|standards|code\s+sets|identifiers?)\b', re.IGNORECASE),
                'sections': re.compile(r'162\.\d+')
            }
        }
    
    def _compile_concept_extractors(self) -> Dict[str, re.Pattern]:
        """Extract key concepts for semantic matching."""
        return {
            'covered_entities': re.compile(r'\b(?:covered\s+entit(?:y|ies)|health\s+plan|health\s+care\s+clearinghouse|health\s+care\s+provider)\b', re.IGNORECASE),
            'business_associates': re.compile(r'\b(?:business\s+associate|business\s+partner)\b', re.IGNORECASE),
            'phi': re.compile(r'\b(?:protected\s+health\s+information|individually\s+identifiable\s+health\s+information|phi|iihi)\b', re.IGNORECASE),
            'minimum_necessary': re.compile(r'\bminimum\s+necessary\b', re.IGNORECASE),
            'authorization': re.compile(r'\bauthorization\b', re.IGNORECASE),
            'disclosure': re.compile(r'\b(?:disclos(?:e|ure)|uses?\s+and\s+disclosures?)\b', re.IGNORECASE),
            'law_enforcement': re.compile(r'\b(?:law\s+enforcement|legal\s+proceedings?|court\s+orders?)\b', re.IGNORECASE),
            'family_members': re.compile(r'\b(?:family\s+members?|relatives?|personal\s+representatives?)\b', re.IGNORECASE),
            'encryption': re.compile(r'\b(?:encrypt(?:ion|ed)?|cryptographic)\b', re.IGNORECASE),
            'breach': re.compile(r'\b(?:breach|security\s+incident)\b', re.IGNORECASE)
        }
    
    def parse_hipaa_regulations(self, input_file: str, output_file: str = None) -> List[EnhancedChunk]:
        """
        Parse HIPAA regulations with enhanced accuracy.
        
        Args:
            input_file: Path to hipaa_regulations.txt
            output_file: Optional output path for enhanced chunks
            
        Returns:
            List of enhanced chunks optimized for RAG
        """
        logger.info(f"üöÄ Starting enhanced HIPAA parsing: {input_file}")
        start_time = datetime.now()
        
        with open(input_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        logger.info(f"üìñ Read {len(content):,} characters")
        
        # Parse into structured sections
        raw_sections = self._extract_structured_sections(content)
        logger.info(f"üìë Extracted {len(raw_sections)} raw sections")
        
        # Create enhanced chunks
        enhanced_chunks = []
        chunk_id = 0
        
        for section in raw_sections:
            section_chunks = self._create_enhanced_chunks(section, chunk_id)
            enhanced_chunks.extend(section_chunks)
            chunk_id += len(section_chunks)
        
        # Post-process and validate
        enhanced_chunks = self._post_process_chunks(enhanced_chunks)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚úÖ Created {len(enhanced_chunks)} enhanced chunks in {processing_time:.2f}s")
        logger.info(f"üìä Stats: {self.parsing_stats}")
        
        if output_file:
            self._save_enhanced_chunks(enhanced_chunks, output_file)
        
        return enhanced_chunks
    
    def _extract_structured_sections(self, content: str) -> List[Dict[str, Any]]:
        """Extract sections with enhanced structure detection."""
        sections = []
        
        # Split by major section boundaries
        section_boundaries = [
            r'^PART\s+\d+',
            r'^SUBPART\s+[A-Z]',
            r'^[¬ß¬ß]\s*\d+\.\d+',
            r'^##\s+[¬ß¬ß]\s*\d+\.\d+'
        ]
        
        pattern = '|'.join(f'({p})' for p in section_boundaries)
        boundary_regex = re.compile(pattern, re.MULTILINE | re.IGNORECASE)
        
        matches = list(boundary_regex.finditer(content))
        
        for i, match in enumerate(matches):
            start_pos = match.start()
            end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(content)
            
            section_content = content[start_pos:end_pos].strip()
            if len(section_content) < 50:  # Skip very short sections
                continue
                
            section_info = self._analyze_section_structure(section_content)
            if section_info:
                sections.append(section_info)
        
        return sections
    
    def _analyze_section_structure(self, section_content: str) -> Optional[Dict[str, Any]]:
        """Analyze section structure with enhanced metadata."""
        lines = section_content.split('\n')
        first_line = lines[0].strip()
        
        # Identify section type and metadata
        section_type, section_id, title = self._identify_enhanced_section_type(first_line)
        if not section_type:
            return None
        
        # Classify regulation domain
        domain = self._classify_regulation_domain(section_content)
        
        # Extract key concepts
        concepts = self._extract_key_concepts(section_content)
        
        # Identify content categories
        categories = self._identify_content_categories(section_content)
        
        # Build CFR citation
        cfr_citation = self._build_cfr_citation(section_type, section_id, title)
        
        return {
            'content': section_content,
            'section_type': section_type,
            'section_id': section_id,
            'title': title,
            'domain': domain,
            'concepts': concepts,
            'categories': categories,
            'cfr_citation': cfr_citation,
            'word_count': len(section_content.split()),
            'char_count': len(section_content)
        }
    
    def _identify_enhanced_section_type(self, first_line: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """Enhanced section type identification."""
        
        # Check for PART
        part_match = self.section_patterns['part'].search(first_line)
        if part_match:
            return 'part', f"PART_{part_match.group(1)}", part_match.group(2).strip()
        
        # Check for SUBPART with specific handling for Privacy/Security
        subpart_match = self.section_patterns['subpart'].search(first_line)
        if subpart_match:
            subpart_id = f"SUBPART_{subpart_match.group(1)}"
            title = subpart_match.group(2).strip()
            
            # Special handling for key subparts
            if 'PRIVACY' in title.upper():
                subpart_id += "_PRIVACY"
            elif 'SECURITY' in title.upper():
                subpart_id += "_SECURITY"
                
            return 'subpart', subpart_id, title
        
        # Check for SECTION
        section_match = self.section_patterns['section'].search(first_line)
        if section_match:
            section_id = f"{section_match.group(1)}.{section_match.group(2)}"
            return 'section', section_id, section_match.group(3).strip()
        
        # Check for subsection header
        subsection_match = self.section_patterns['subsection_header'].search(first_line)
        if subsection_match:
            section_id = f"{subsection_match.group(1)}.{subsection_match.group(2)}"
            return 'section', section_id, subsection_match.group(3).strip()
        
        return None, None, None
    
    def _classify_regulation_domain(self, content: str) -> str:
        """Classify the regulatory domain with enhanced accuracy."""
        scores = {}
        
        for domain, patterns in self.domain_classifiers.items():
            score = 0
            
            # Keyword matching
            if 'keywords' in patterns:
                score += len(patterns['keywords'].findall(content)) * 2
            
            # Section number matching
            if 'sections' in patterns:
                score += len(patterns['sections'].findall(content)) * 5
            
            # Subpart matching
            if 'subpart' in patterns:
                if patterns['subpart'].search(content):
                    score += 10
            
            scores[domain] = score
        
        # Return domain with highest score, default to 'general'
        return max(scores.items(), key=lambda x: x[1])[0] if any(scores.values()) else 'general'
    
    def _extract_key_concepts(self, content: str) -> Set[str]:
        """Extract key concepts for semantic matching."""
        concepts = set()
        
        for concept, pattern in self.concept_extractors.items():
            if pattern.search(content):
                concepts.add(concept)
        
        return concepts
    
    def _identify_content_categories(self, content: str) -> Set[str]:
        """Identify content categories."""
        categories = set()
        
        # Check for definitions
        if re.search(r'\b(?:means|definition|defined as)\b', content, re.IGNORECASE):
            categories.add('definitions')
        
        # Check for requirements
        if re.search(r'\b(?:must|shall|required|implement)\b', content, re.IGNORECASE):
            categories.add('requirements')
        
        # Check for penalties
        if re.search(r'\b(?:penalty|fine|violation|civil money)\b', content, re.IGNORECASE):
            categories.add('penalties')
        
        # Check for exceptions
        if re.search(r'\b(?:exception|unless|except|does not apply)\b', content, re.IGNORECASE):
            categories.add('exceptions')
        
        return categories
    
    def _build_cfr_citation(self, section_type: str, section_id: str, title: str) -> str:
        """Build proper CFR citation."""
        if section_type == 'part':
            part_num = section_id.replace('PART_', '')
            return f"45 CFR Part {part_num}"
        elif section_type == 'subpart':
            # Extract part and subpart
            if '_' in section_id:
                parts = section_id.split('_')
                if len(parts) >= 2:
                    subpart_letter = parts[1]
                    return f"45 CFR Part 164, Subpart {subpart_letter}"
        elif section_type == 'section':
            return f"45 CFR ¬ß {section_id}"
        
        return f"45 CFR {section_id}"
    
    def _create_enhanced_chunks(self, section: Dict[str, Any], start_chunk_id: int) -> List[EnhancedChunk]:
        """Create enhanced chunks with optimal sizing."""
        content = section['content']
        
        # Determine chunk strategy based on content length and type
        if len(content) <= 800:  # Small section - keep as single chunk
            chunks = [content]
        elif section['section_type'] == 'section' and 'definitions' in section['categories']:
            # Definition sections - split by term
            chunks = self._split_definitions_section(content)
        else:
            # Large section - split semantically
            chunks = self._split_section_semantically(content)
        
        enhanced_chunks = []
        for i, chunk_content in enumerate(chunks):
            metadata = self._create_enhanced_metadata(
                section, i, len(chunks), start_chunk_id + i
            )
            
            enhanced_chunks.append(EnhancedChunk(
                chunk_id=start_chunk_id + i,
                content=chunk_content.strip(),
                metadata=metadata
            ))
            
            # Update stats
            self.parsing_stats['total_chunks'] += 1
            if section['domain'] == 'privacy':
                self.parsing_stats['privacy_chunks'] += 1
            elif section['domain'] == 'security':
                self.parsing_stats['security_chunks'] += 1
            
            if 'definitions' in section['categories']:
                self.parsing_stats['definition_chunks'] += 1
            if 'penalties' in section['categories']:
                self.parsing_stats['penalty_chunks'] += 1
            if 'requirements' in section['categories']:
                self.parsing_stats['requirement_chunks'] += 1
        
        return enhanced_chunks
    
    def _split_definitions_section(self, content: str) -> List[str]:
        """Split definitions section by individual terms."""
        chunks = []
        
        # Look for definition patterns
        definition_pattern = re.compile(r'^([A-Z][A-Za-z\s]+?)\s+means\s+(.+?)(?=\n[A-Z][A-Za-z\s]+?\s+means|\Z)', 
                                       re.MULTILINE | re.DOTALL)
        
        matches = list(definition_pattern.finditer(content))
        
        if matches:
            # Split by definitions
            last_end = 0
            for match in matches:
                # Include any intro text before first definition
                if match.start() > last_end:
                    intro = content[last_end:match.start()].strip()
                    if intro and len(intro) > 50:
                        chunks.append(intro)
                
                # Add the definition
                definition_text = match.group(0).strip()
                chunks.append(definition_text)
                last_end = match.end()
            
            # Add any remaining content
            if last_end < len(content):
                remaining = content[last_end:].strip()
                if remaining and len(remaining) > 50:
                    chunks.append(remaining)
        else:
            # No clear definitions pattern - split by paragraphs
            chunks = self._split_by_paragraphs(content)
        
        return [chunk for chunk in chunks if len(chunk.strip()) > 50]
    
    def _split_section_semantically(self, content: str) -> List[str]:
        """Split section content semantically."""
        
        # First try to split by subsections
        subsection_pattern = re.compile(r'\n\(([a-z]|\d+)\)\s+', re.MULTILINE)
        subsection_splits = subsection_pattern.split(content)
        
        if len(subsection_splits) > 1:
            chunks = []
            for i in range(0, len(subsection_splits), 2):
                if i + 1 < len(subsection_splits):
                    subsection_id = subsection_splits[i + 1]
                    subsection_content = subsection_splits[i + 2] if i + 2 < len(subsection_splits) else ""
                    chunk = f"({subsection_id}) {subsection_content}".strip()
                    if len(chunk) > 50:
                        chunks.append(chunk)
                elif subsection_splits[i].strip():
                    chunks.append(subsection_splits[i].strip())
            
            return chunks
        else:
            # Fall back to paragraph splitting
            return self._split_by_paragraphs(content)
    
    def _split_by_paragraphs(self, content: str, max_chunk_size: int = 800) -> List[str]:
        """Split content by paragraphs with size limits."""
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) + 2 <= max_chunk_size:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = paragraph
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _create_enhanced_metadata(self, section: Dict[str, Any], chunk_index: int, 
                                 total_chunks: int, chunk_id: int) -> EnhancedSectionMetadata:
        """Create enhanced metadata for precise retrieval."""
        
        return EnhancedSectionMetadata(
            section_id=section['section_id'],
            section_type=section['section_type'],
            section_title=section['title'],
            full_reference=f"45 CFR {section['section_id']} - {section['title']}",
            cfr_citation=section['cfr_citation'],
            parent_section=None,  # Could be enhanced
            hierarchy_level=self._get_hierarchy_level(section['section_type']),
            regulation_domain=section['domain'],
            content_categories=section['categories'],
            key_concepts=section['concepts'],
            word_count=len(section['content'].split()),
            character_count=len(section['content']),
            chunk_index=chunk_index,
            total_chunks=total_chunks,
            exact_quote_ranges=[],  # Could be enhanced
            subsection_map={}  # Could be enhanced
        )
    
    def _get_hierarchy_level(self, section_type: str) -> int:
        """Get hierarchy level for section type."""
        hierarchy_map = {
            'part': 1,
            'subpart': 2,
            'section': 3,
            'subsection': 4,
            'paragraph': 5
        }
        return hierarchy_map.get(section_type, 3)
    
    def _post_process_chunks(self, chunks: List[EnhancedChunk]) -> List[EnhancedChunk]:
        """Post-process chunks for optimization."""
        logger.info("üîß Post-processing chunks for optimization...")
        
        # Remove very short chunks that don't add value
        filtered_chunks = []
        for chunk in chunks:
            if len(chunk.content.split()) >= 10:  # Minimum 10 words
                filtered_chunks.append(chunk)
            else:
                logger.debug(f"Filtered out short chunk: {chunk.chunk_id}")
        
        # Renumber chunk IDs
        for i, chunk in enumerate(filtered_chunks):
            chunk.chunk_id = i
        
        logger.info(f"üìã Kept {len(filtered_chunks)} of {len(chunks)} chunks after filtering")
        return filtered_chunks
    
    def _save_enhanced_chunks(self, chunks: List[EnhancedChunk], output_file: str):
        """Save enhanced chunks to JSON file."""
        chunk_data = []
        
        for chunk in chunks:
            chunk_dict = {
                'chunk_id': chunk.chunk_id,
                'content': chunk.content,
                'metadata': {
                    'section_id': chunk.metadata.section_id,
                    'section_type': chunk.metadata.section_type,
                    'section_title': chunk.metadata.section_title,
                    'full_reference': chunk.metadata.full_reference,
                    'cfr_citation': chunk.metadata.cfr_citation,
                    'parent_section': chunk.metadata.parent_section,
                    'hierarchy_level': chunk.metadata.hierarchy_level,
                    'regulation_domain': chunk.metadata.regulation_domain,
                    'content_categories': list(chunk.metadata.content_categories),
                    'key_concepts': list(chunk.metadata.key_concepts),
                    'word_count': chunk.metadata.word_count,
                    'character_count': chunk.metadata.character_count,
                    'chunk_index': chunk.metadata.chunk_index,
                    'total_chunks': chunk.metadata.total_chunks
                }
            }
            chunk_data.append(chunk_dict)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunk_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ Saved {len(chunks)} enhanced chunks to {output_file}")


def main():
    """Main function for command line usage."""
    parser = argparse.ArgumentParser(description='Enhanced HIPAA Parser for RAG Systems')
    parser.add_argument('input_file', help='Input HIPAA regulations text file')
    parser.add_argument('-o', '--output', help='Output JSON file for enhanced chunks')
    
    args = parser.parse_args()
    
    enhanced_parser = EnhancedHIPAAParser()
    chunks = enhanced_parser.parse_hipaa_regulations(
        args.input_file, 
        args.output or 'enhanced_hipaa_chunks.json'
    )
    
    print(f"‚úÖ Successfully created {len(chunks)} enhanced chunks")
    print(f"üìä Processing stats: {enhanced_parser.parsing_stats}")


if __name__ == "__main__":
    main()

--------------------------------------------------
File End
--------------------------------------------------


scripts/__init__.py
File type: .py
# Scripts package

--------------------------------------------------
File End
--------------------------------------------------


scripts/cleanup.py
File type: .py
#!/usr/bin/env python3
"""
Repository cleanup script for HIPAA QA System.
Removes temporary files, caches, and build artifacts.
"""

import os
import shutil
import glob
from pathlib import Path

def cleanup_repository():
    """Clean up temporary files and build artifacts."""
    
    print("üßπ Cleaning up repository...")
    
    # Get repository root
    repo_root = Path(__file__).parent.parent
    os.chdir(repo_root)
    
    removed_count = 0
    
    # Python cache and compiled files
    patterns_to_remove = [
        "**/__pycache__",
        "**/*.pyc", 
        "**/*.pyo",
        "**/*.pyd",
        "**/.pytest_cache",
        "**/*.egg-info",
        "**/build",
        "**/dist"
    ]
    
    for pattern in patterns_to_remove:
        for path in glob.glob(pattern, recursive=True):
            try:
                if os.path.isdir(path):
                    shutil.rmtree(path)
                    print(f"  Removed directory: {path}")
                else:
                    os.remove(path)
                    print(f"  Removed file: {path}")
                removed_count += 1
            except OSError as e:
                print(f"  Warning: Could not remove {path}: {e}")
    
    # Temporary and backup files
    temp_patterns = [
        "*.tmp",
        "*.bak", 
        "*.orig",
        "*~",
        "*.log",
        ".DS_Store",
        "Thumbs.db"
    ]
    
    for pattern in temp_patterns:
        for path in glob.glob(pattern, recursive=True):
            try:
                os.remove(path)
                print(f"  Removed temp file: {path}")
                removed_count += 1
            except OSError as e:
                print(f"  Warning: Could not remove {path}: {e}")
    
    # Clean logs directory but keep structure
    logs_dir = Path("logs")
    if logs_dir.exists():
        for log_file in logs_dir.glob("*"):
            if log_file.name != ".gitkeep":
                try:
                    log_file.unlink()
                    print(f"  Removed log: {log_file}")
                    removed_count += 1
                except OSError as e:
                    print(f"  Warning: Could not remove {log_file}: {e}")
    
    # Ensure .gitkeep exists in logs
    gitkeep_path = logs_dir / ".gitkeep"
    if not gitkeep_path.exists():
        gitkeep_path.touch()
        print(f"  Created: {gitkeep_path}")
    
    # Clean test results and generated files
    test_artifacts = [
        "enhanced_test_results.json",
        "enhanced_ingestion.log", 
        "enhanced_parser.log",
        "pdf_extraction.log",
        "section_parser.log"
    ]
    
    for artifact in test_artifacts:
        if os.path.exists(artifact):
            try:
                os.remove(artifact)
                print(f"  Removed test artifact: {artifact}")
                removed_count += 1
            except OSError as e:
                print(f"  Warning: Could not remove {artifact}: {e}")
    
    print(f"\n‚úÖ Cleanup complete! Removed {removed_count} items.")
    print("\nüìù Files preserved:")
    print("  ‚Ä¢ Source code (.py files)")
    print("  ‚Ä¢ Configuration files") 
    print("  ‚Ä¢ Documentation")
    print("  ‚Ä¢ Docker files")
    print("  ‚Ä¢ Data structure (empty directories with .gitkeep)")

if __name__ == "__main__":
    cleanup_repository()

--------------------------------------------------
File End
--------------------------------------------------


scripts/pdf_extractor.py
File type: .py
"""Async Production PDF Extractor for HIPAA QA System.

This module provides an async, production-ready PDF extraction service
using Mistral's OCR API with comprehensive error handling, logging,
and configuration management.
"""

import asyncio
import logging
import os
from pathlib import Path
from typing import Optional, Dict, Any, Callable, List
from dataclasses import dataclass
import json
from datetime import datetime
import aiofiles
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from mistralai import Mistral
from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk
import dotenv

# Load environment variables
dotenv.load_dotenv()


logger = logging.getLogger(__name__)


@dataclass
class ExtractionConfig:
    """Configuration for PDF extraction process."""
    input_pdf_path: Path
    output_text_path: Path
    mistral_api_key: str
    model: str = "mistral-ocr-latest"
    include_image_base64: bool = False
    signed_url_expiry_hours: int = 1
    max_retries: int = 3
    timeout_seconds: int = 300
    chunk_size: int = 1024 * 1024  # 1MB chunks for file upload

    @classmethod
    def from_env(cls, input_pdf_path: str, output_text_path: str) -> "ExtractionConfig":
        """Create configuration from environment variables."""
        api_key = os.getenv("MISTRAL_API_KEY")
        if not api_key:
            raise ValueError("MISTRAL_API_KEY environment variable is required")
        
        return cls(
            input_pdf_path=Path(input_pdf_path),
            output_text_path=Path(output_text_path),
            mistral_api_key=api_key,
            model=os.getenv("MISTRAL_OCR_MODEL", "mistral-ocr-latest"),
            max_retries=int(os.getenv("MISTRAL_MAX_RETRIES", "3")),
            timeout_seconds=int(os.getenv("MISTRAL_TIMEOUT_SECONDS", "300"))
        )


@dataclass
class ExtractionProgress:
    """Progress tracking for PDF extraction."""
    stage: str
    progress_percent: float
    message: str
    timestamp: datetime
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class ExtractionResult:
    """Result of PDF extraction process."""
    success: bool
    extracted_text: Optional[str]
    output_file_path: Optional[Path]
    error_message: Optional[str]
    processing_time_seconds: float
    pages_processed: int
    file_size_bytes: int
    metadata: Dict[str, Any]


class PDFExtractionError(Exception):
    """Custom exception for PDF extraction errors."""
    pass


class AsyncPDFExtractor:
    """Async production-ready PDF extractor using Mistral OCR."""
    
    def __init__(self, config: ExtractionConfig, progress_callback: Optional[Callable[[ExtractionProgress], None]] = None):
        self.config = config
        self.progress_callback = progress_callback
        self.client = Mistral(api_key=config.mistral_api_key)
        self._validate_config()
    
    def _validate_config(self) -> None:
        """Validate configuration parameters."""
        if not self.config.input_pdf_path.exists():
            raise PDFExtractionError(f"Input PDF file does not exist: {self.config.input_pdf_path}")
        
        if not self.config.input_pdf_path.suffix.lower() == '.pdf':
            raise PDFExtractionError(f"Input file is not a PDF: {self.config.input_pdf_path}")
        
        # Ensure output directory exists
        self.config.output_text_path.parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"‚úÖ Configuration validated for PDF extraction: {self.config.input_pdf_path}")
    
    def _report_progress(self, stage: str, progress: float, message: str, metadata: Optional[Dict[str, Any]] = None) -> None:
        """Report progress if callback is provided."""
        if self.progress_callback:
            progress_info = ExtractionProgress(
                stage=stage,
                progress_percent=progress,
                message=message,
                timestamp=datetime.now(),
                metadata=metadata or {}
            )
            self.progress_callback(progress_info)
        
        logger.info(f"üìä [{stage}] {progress:.1f}% - {message}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((httpx.RequestError, PDFExtractionError))
    )
    async def _upload_pdf_with_retry(self) -> Any:
        """Upload PDF file to Mistral with retry logic."""
        try:
            self._report_progress("upload", 10, "Reading PDF file")
            
            # Read file asynchronously
            async with aiofiles.open(self.config.input_pdf_path, 'rb') as f:
                file_content = await f.read()
            
            file_size_mb = len(file_content) / (1024 * 1024)
            self._report_progress("upload", 30, f"Uploading PDF file ({file_size_mb:.1f} MB)", 
                                {"file_size_bytes": len(file_content)})
            
            # Upload file
            uploaded_file = self.client.files.upload(
                file={
                    "file_name": self.config.input_pdf_path.stem,
                    "content": file_content,
                },
                purpose="ocr",
            )
            
            self._report_progress("upload", 60, f"File uploaded successfully. ID: {uploaded_file.id}")
            return uploaded_file
            
        except Exception as e:
            error_msg = f"Failed to upload PDF: {str(e)}"
            logger.error(error_msg)
            raise PDFExtractionError(error_msg) from e
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((httpx.RequestError, PDFExtractionError))
    )
    async def _process_ocr_with_retry(self, uploaded_file: Any) -> Any:
        """Process OCR with retry logic."""
        try:
            self._report_progress("processing", 70, "Getting signed URL for processing")
            
            # Get signed URL
            signed_url = self.client.files.get_signed_url(
                file_id=uploaded_file.id, 
                expiry=self.config.signed_url_expiry_hours
            )
            
            self._report_progress("processing", 80, "Starting OCR processing")
            
            # Process OCR
            pdf_response = self.client.ocr.process(
                document=DocumentURLChunk(document_url=signed_url.url),
                model=self.config.model,
                include_image_base64=self.config.include_image_base64
            )
            
            self._report_progress("processing", 90, f"OCR completed. Pages processed: {len(pdf_response.pages)}")
            return pdf_response
            
        except Exception as e:
            error_msg = f"Failed to process OCR: {str(e)}"
            logger.error(error_msg)
            raise PDFExtractionError(error_msg) from e
    
    async def _save_results(self, pdf_response: Any) -> int:
        """Save OCR results to file asynchronously."""
        try:
            self._report_progress("saving", 95, "Saving OCR results to file")
            
            async with aiofiles.open(self.config.output_text_path, "w", encoding="utf-8") as f:
                for i, page in enumerate(pdf_response.pages):
                    await f.write(page.markdown)
                    await f.write("\n\n---\n\n")  # Add separator between pages
                    
                    if i % 10 == 0:  # Progress update every 10 pages
                        progress = 95 + (i / len(pdf_response.pages)) * 5
                        self._report_progress("saving", progress, f"Saved page {i+1}/{len(pdf_response.pages)}")
            
            pages_count = len(pdf_response.pages)
            self._report_progress("saving", 100, f"Successfully saved {pages_count} pages to {self.config.output_text_path}")
            
            return pages_count
            
        except Exception as e:
            error_msg = f"Failed to save OCR results: {str(e)}"
            logger.error(error_msg)
            raise PDFExtractionError(error_msg) from e
    
    async def extract_async(self) -> ExtractionResult:
        """Extract text from PDF asynchronously with comprehensive error handling."""
        start_time = datetime.now()
        file_size = self.config.input_pdf_path.stat().st_size
        
        try:
            logger.info(f"üöÄ Starting async PDF extraction: {self.config.input_pdf_path}")
            self._report_progress("init", 0, "Starting PDF extraction")
            
            # Upload PDF
            uploaded_file = await self._upload_pdf_with_retry()
            
            # Process OCR
            pdf_response = await self._process_ocr_with_retry(uploaded_file)
            
            # Save results
            pages_processed = await self._save_results(pdf_response)
            
            # Calculate processing time
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # Extract text from all pages
            extracted_text = ""
            for page in pdf_response.pages:
                extracted_text += page.markdown + "\n\n---\n\n"
            
            result = ExtractionResult(
                success=True,
                extracted_text=extracted_text.strip(),
                output_file_path=self.config.output_text_path,
                error_message=None,
                processing_time_seconds=processing_time,
                pages_processed=pages_processed,
                file_size_bytes=file_size,
                metadata={
                    "model_used": self.config.model,
                    "input_file": str(self.config.input_pdf_path),
                    "output_file": str(self.config.output_text_path),
                    "extraction_timestamp": start_time.isoformat()
                }
            )
            
            logger.info(f"‚úÖ PDF extraction completed successfully in {processing_time:.2f}s")
            return result
            
        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            error_msg = f"PDF extraction failed: {str(e)}"
            logger.error(error_msg)
            
            result = ExtractionResult(
                success=False,
                extracted_text=None,
                output_file_path=None,
                error_message=error_msg,
                processing_time_seconds=processing_time,
                pages_processed=0,
                file_size_bytes=file_size,
                metadata={
                    "error_type": type(e).__name__,
                    "input_file": str(self.config.input_pdf_path),
                    "extraction_timestamp": start_time.isoformat()
                }
            )
            
            return result
    
    async def cleanup(self) -> None:
        """Cleanup any temporary resources."""
        # Add any cleanup logic here if needed
        logger.info("üßπ Cleanup completed")


# Legacy sync wrapper for backward compatibility
def extract_txt(pdf_path: str) -> str:
    """Legacy synchronous wrapper for backward compatibility.
    
    WARNING: This function is deprecated. Use AsyncPDFExtractor for new code.
    """
    logger.warning("extract_txt() is deprecated. Use AsyncPDFExtractor for new code.")
    
    config = ExtractionConfig.from_env(
        input_pdf_path=pdf_path,
        output_text_path="data/clean/hipaa_regulations.txt"
    )
    
    extractor = AsyncPDFExtractor(config)
    
    # Run async function in sync context
    result = asyncio.run(extractor.extract_async())
    
    if not result.success:
        raise PDFExtractionError(result.error_message)
    
    return result.extracted_text or ""

async def main():
    """Main function for running PDF extraction."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Extract text from PDF using Mistral OCR")
    parser.add_argument(
        "--input", 
        type=str, 
        default="data/raw/hipaa-combined (2).pdf",
        help="Input PDF file path"
    )
    parser.add_argument(
        "--output", 
        type=str, 
        default="data/clean/hipaa_regulations.txt",
        help="Output text file path"
    )
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="Enable verbose logging"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    def progress_callback(progress: ExtractionProgress):
        """Progress callback for reporting extraction progress."""
        print(f"[{progress.stage.upper()}] {progress.progress_percent:.1f}% - {progress.message}")
    
    try:
        # Create configuration
        config = ExtractionConfig.from_env(
            input_pdf_path=args.input,
            output_text_path=args.output
        )
        
        # Create extractor with progress callback
        extractor = AsyncPDFExtractor(config, progress_callback=progress_callback)
        
        # Extract text
        result = await extractor.extract_async()
        
        # Cleanup
        await extractor.cleanup()
        
        if result.success:
            print(f"\n‚úÖ Success! Extracted {result.pages_processed} pages in {result.processing_time_seconds:.2f}s")
            print(f"üìÑ Output saved to: {result.output_file_path}")
            print(f"üìä File size: {result.file_size_bytes / (1024*1024):.1f} MB")
            
            # Print first 500 characters as preview
            if result.extracted_text:
                preview = result.extracted_text[:500] + "..." if len(result.extracted_text) > 500 else result.extracted_text
                print(f"\nüìñ Preview:\n{preview}")
        else:
            print(f"\n‚ùå Extraction failed: {result.error_message}")
            return 1
            
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

--------------------------------------------------
File End
--------------------------------------------------


scripts/run_dev.sh
File type: .sh
#!/bin/bash

# Development script to run HIPAA QA System locally

set -e

echo "üè• Starting HIPAA QA System in development mode..."

# Check if .env file exists
if [ ! -f .env ]; then
    echo "‚ùå .env file not found. Please copy env.example to .env and configure your settings."
    exit 1
fi

# Check if OpenAI API key is set
if ! grep -q "^OPENAI_API_KEY=sk-" .env; then
    echo "‚ùå OPENAI_API_KEY not set in .env file. Please add your OpenAI API key."
    exit 1
fi

# Create logs directory
mkdir -p logs

# Build and start services
echo "üê≥ Building Docker images..."
docker-compose build

echo "üìä Starting database..."
docker-compose up -d db

echo "‚è≥ Waiting for database to be ready..."
sleep 10

# Check if data needs to be ingested
echo "üì• Checking if data ingestion is needed..."
if docker-compose exec -T db psql -U postgres -d hipaa_qa -c "SELECT COUNT(*) FROM document_chunks;" 2>/dev/null | grep -q " 0"; then
    echo "üì¶ No data found. Running ingestion..."
    docker-compose run --rm backend python /app/scripts/ingest_data.py
else
    echo "‚úÖ Data already exists in database"
fi

echo "üöÄ Starting all services..."
docker-compose up -d

echo "‚è≥ Waiting for services to be ready..."
sleep 15

echo ""
echo "üéâ HIPAA QA System is running!"
echo ""
echo "üìä Services:"
echo "  - Backend API: http://localhost:8000"
echo "  - API Docs: http://localhost:8000/docs"
echo "  - Frontend UI: http://localhost:7860"
echo "  - Database: localhost:5432"
echo ""
echo "üåê Public URL (via Cloudflare Tunnel):"
echo "  Check the cloudflared container logs for the public URL:"
echo "  docker-compose logs cloudflared | grep trycloudflare.com"
echo ""
echo "üìä Useful commands:"
echo "  - View logs: docker-compose logs -f [service]"
echo "  - Stop system: docker-compose down"
echo "  - Restart: docker-compose restart [service]"
echo "  - Health check: curl http://localhost:8000/health"
echo ""

# Show the Cloudflare tunnel URL
echo "üîó Getting Cloudflare tunnel URL..."
sleep 5
docker-compose logs cloudflared 2>/dev/null | grep -o "https://[a-zA-Z0-9-]*\.trycloudflare\.com" | tail -1 | xargs -I {} echo "  Public URL: {}"

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/config.py
File type: .py
"""Configuration management for HIPAA QA System."""

from functools import lru_cache
from typing import Optional

from pydantic import Field, validator
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """Application settings with validation and environment variable support."""
    
    # OpenAI Configuration
    openai_api_key: str = Field(..., description="OpenAI API key for embeddings and chat")
    openai_embedding_model: str = Field(
        default="text-embedding-3-large",
        description="OpenAI embedding model to use"
    )
    openai_chat_model: str = Field(
        default="gpt-4",
        description="OpenAI chat model for question answering"
    )
    openai_max_retries: int = Field(default=3, description="Max retries for OpenAI API calls")
    openai_timeout: int = Field(default=30, description="Timeout for OpenAI API calls in seconds")
    
    # Database Configuration
    db_host: str = Field(default="localhost", description="PostgreSQL host", alias="DB_HOST")
    db_port: int = Field(default=5432, description="PostgreSQL port", alias="DB_PORT")
    db_name: str = Field(default="hipaa_qa", description="Database name", alias="POSTGRES_DB")
    db_user: str = Field(default="postgres", description="Database user", alias="POSTGRES_USER")
    db_password: str = Field(default="", description="Database password", alias="POSTGRES_PASSWORD")
    db_pool_size: int = Field(default=10, description="Database connection pool size")
    db_max_overflow: int = Field(default=20, description="Database connection pool max overflow")
    
    # Vector Search Configuration
    embedding_dimension: int = Field(
        default=3072, 
        description="Embedding vector dimension for text-embedding-3-large"
    )
    similarity_threshold: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Minimum similarity score for relevant chunks"
    )
    max_chunks_retrieved: int = Field(
        default=5,
        ge=1,
        le=20,
        description="Maximum number of chunks to retrieve for context"
    )
    chunk_overlap_tokens: int = Field(
        default=50,
        description="Number of overlapping tokens between chunks"
    )
    
    # API Configuration
    api_host: str = Field(default="0.0.0.0", description="FastAPI host")
    api_port: int = Field(default=8000, description="FastAPI port", alias="API_PORT")
    api_workers: int = Field(default=1, description="Number of uvicorn workers")
    api_debug: bool = Field(default=False, description="Enable debug mode", alias="DEBUG")
    api_cors_origins: list[str] = Field(
        default=["*"],
        description="Allowed CORS origins"
    )
    
    # Frontend Configuration
    gradio_host: str = Field(default="0.0.0.0", description="Gradio host")
    gradio_port: int = Field(default=7860, description="Gradio port", alias="FRONTEND_PORT")
    gradio_share: bool = Field(default=False, description="Enable Gradio sharing")
    
    # Logging Configuration
    log_level: str = Field(default="INFO", description="Logging level")
    log_format: str = Field(
        default="json",
        description="Log format: 'json' or 'text'"
    )
    
    # Data Paths
    data_path: str = Field(default="data", description="Base data directory path")
    chunks_file: str = Field(
        default="data/clean/hipaa_rag_chunks.json",
        description="Path to processed chunks file"
    )
    
    # Performance Configuration
    batch_size: int = Field(default=100, description="Batch size for embedding operations")
    request_timeout: int = Field(default=60, description="Request timeout in seconds")
    
    @validator("openai_api_key", pre=True)
    def validate_openai_key(cls, v: str) -> str:
        """Validate OpenAI API key format."""
        if not v.startswith("sk-"):
            raise ValueError("OpenAI API key must start with 'sk-'")
        return v
        
    @validator("log_level", pre=True)
    def validate_log_level(cls, v: str) -> str:
        """Validate log level."""
        valid_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        if v.upper() not in valid_levels:
            raise ValueError(f"Log level must be one of: {valid_levels}")
        return v.upper()
        
    @validator("log_format", pre=True)
    def validate_log_format(cls, v: str) -> str:
        """Validate log format."""
        valid_formats = ["json", "text"]
        if v.lower() not in valid_formats:
            raise ValueError(f"Log format must be one of: {valid_formats}")
        return v.lower()

    @property
    def database_url(self) -> str:
        """Get the complete database URL."""
        auth_part = f"{self.db_user}"
        if self.db_password:
            auth_part += f":{self.db_password}"
        return (
            f"postgresql+asyncpg://{auth_part}"
            f"@{self.db_host}:{self.db_port}/{self.db_name}"
        )
        
    @property
    def sync_database_url(self) -> str:
        """Get the synchronous database URL for migrations."""
        auth_part = f"{self.db_user}"
        if self.db_password:
            auth_part += f":{self.db_password}"
        return (
            f"postgresql://{auth_part}"
            f"@{self.db_host}:{self.db_port}/{self.db_name}"
        )

    class Config:
        """Pydantic model configuration."""
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False
        env_prefix = ""
        extra = "ignore"  # Allow extra env vars to be ignored


@lru_cache()
def get_settings() -> Settings:
    """Get cached application settings."""
    return Settings()


# Global settings instance
settings = get_settings()

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/__init__.py
File type: .py
"""HIPAA QA System - A RAG-based chatbot for HIPAA regulations."""

__version__ = "0.1.0"
__author__ = "HIPAA QA Team"
__email__ = "team@example.com"
__description__ = "HIPAA Regulation QA System using RAG with OpenAI and pgvector"

from typing import Final

# Package constants
PACKAGE_NAME: Final[str] = "hipaa-qa-system"
OPENAI_EMBEDDING_MODEL: Final[str] = "text-embedding-3-large"
OPENAI_CHAT_MODEL: Final[str] = "gpt-4"
EMBEDDING_DIMENSION: Final[int] = 3072  # text-embedding-3-large dimension
MAX_TOKENS_PER_CHUNK: Final[int] = 500
SIMILARITY_THRESHOLD: Final[float] = 0.7
DEFAULT_TOP_K: Final[int] = 5

__all__ = [
    "__version__",
    "__author__", 
    "__email__",
    "__description__",
    "PACKAGE_NAME",
    "OPENAI_EMBEDDING_MODEL",
    "OPENAI_CHAT_MODEL", 
    "EMBEDDING_DIMENSION",
    "MAX_TOKENS_PER_CHUNK",
    "SIMILARITY_THRESHOLD",
    "DEFAULT_TOP_K",
]

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/schemas.py
File type: .py
"""Pydantic schemas for HIPAA QA System API contracts."""

from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Union

from pydantic import BaseModel, Field, validator


class ContentType(str, Enum):
    """Types of regulatory content."""
    DEFINITION = "definition"
    REQUIREMENT = "requirement"
    GENERAL = "general"
    PENALTY = "penalty"
    PROCEDURE = "procedure"


class ComplianceLevel(str, Enum):
    """Compliance level indicators."""
    MANDATORY = "mandatory"
    REQUIRED = "required"
    PERMITTED = "permitted"
    PROHIBITED = "prohibited"
    INFORMATIONAL = "informational"


class SectionType(str, Enum):
    """HIPAA regulation section types."""
    PART = "part"
    SUBPART = "subpart"
    SECTION = "section"
    SUBSECTION = "subsection"
    PARAGRAPH = "paragraph"


class ChunkMetadata(BaseModel):
    """Metadata for a regulation text chunk."""
    
    section_id: str = Field(..., description="Unique section identifier")
    section_type: SectionType = Field(..., description="Type of regulation section")
    section_title: str = Field(..., description="Title of the section")
    full_reference: str = Field(..., description="Complete CFR reference")
    cfr_citation: Optional[str] = Field(None, description="Specific CFR citation")
    parent_section: Optional[str] = Field(None, description="Parent section ID")
    hierarchy_level: int = Field(..., ge=1, le=10, description="Hierarchy depth level")
    
    # Chunk-specific metadata
    chunk_index: int = Field(..., ge=0, description="Index of this chunk within section")
    total_chunks: int = Field(..., ge=1, description="Total chunks for this section")
    chunk_size: int = Field(..., ge=0, description="Character count of chunk")
    word_count: int = Field(..., ge=0, description="Word count of chunk")
    
    # Content classification
    contains_definitions: bool = Field(default=False, description="Contains regulatory definitions")
    contains_penalties: bool = Field(default=False, description="Contains penalty information")
    contains_requirements: bool = Field(default=False, description="Contains compliance requirements")
    
    # Reference tracking
    references: List[str] = Field(default_factory=list, description="Referenced sections/citations")
    key_terms: List[str] = Field(default_factory=list, description="Important regulatory terms")
    
    # Classification
    content_type: ContentType = Field(default=ContentType.GENERAL, description="Type of content")
    compliance_level: ComplianceLevel = Field(
        default=ComplianceLevel.INFORMATIONAL,
        description="Compliance importance level"
    )


class DocumentChunk(BaseModel):
    """A chunk of regulatory text with metadata and embeddings."""
    
    chunk_id: int = Field(..., ge=0, description="Unique chunk identifier")
    content: str = Field(..., min_length=1, description="The actual regulation text")
    metadata: ChunkMetadata = Field(..., description="Chunk metadata and classification")
    embedding: Optional[List[float]] = Field(None, description="Vector embedding of the content")
    created_at: Optional[datetime] = Field(default_factory=datetime.utcnow)
    updated_at: Optional[datetime] = Field(default_factory=datetime.utcnow)
    
    @validator("embedding")
    def validate_embedding_dimension(cls, v: Optional[List[float]]) -> Optional[List[float]]:
        """Validate embedding dimension matches expected size."""
        if v is not None and len(v) != 3072:  # text-embedding-3-large dimension
            raise ValueError(f"Embedding must have 3072 dimensions, got {len(v)}")
        return v
        
    @validator("content")
    def validate_content_not_empty(cls, v: str) -> str:
        """Ensure content is not just whitespace."""
        if not v.strip():
            raise ValueError("Content cannot be empty or only whitespace")
        return v.strip()


class QuestionRequest(BaseModel):
    """Request schema for asking a question."""
    
    question: str = Field(..., min_length=5, max_length=500, description="User's question about HIPAA")
    max_chunks: Optional[int] = Field(
        default=5,
        ge=1,
        le=10,
        description="Maximum number of context chunks to retrieve"
    )
    similarity_threshold: Optional[float] = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Minimum similarity threshold for chunk relevance"
    )
    include_metadata: bool = Field(
        default=False,
        description="Include chunk metadata in response"
    )
    
    @validator("question")
    def validate_question_content(cls, v: str) -> str:
        """Validate question is meaningful."""
        cleaned = v.strip()
        if not cleaned:
            raise ValueError("Question cannot be empty")
        if len(cleaned.split()) < 2:
            raise ValueError("Question must contain at least 2 words")
        return cleaned


class SourceReference(BaseModel):
    """Reference to a source regulation section."""
    
    section_id: str = Field(..., description="Section identifier")
    cfr_citation: str = Field(..., description="CFR citation")
    section_title: str = Field(..., description="Section title")
    content_excerpt: str = Field(..., description="Relevant excerpt from the section")
    similarity_score: float = Field(..., ge=0.0, le=1.0, description="Similarity score")
    chunk_id: int = Field(..., description="Source chunk ID")


class AnswerResponse(BaseModel):
    """Response schema for question answers."""
    
    question: str = Field(..., description="Original question")
    answer: str = Field(..., description="Generated answer with citations")
    sources: List[SourceReference] = Field(
        default_factory=list,
        description="Source references used to generate the answer"
    )
    confidence_score: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description="Confidence in the answer quality"
    )
    processing_time_ms: Optional[int] = Field(
        None,
        ge=0,
        description="Time taken to process the request in milliseconds"
    )
    model_used: str = Field(..., description="OpenAI model used for generation")
    chunks_retrieved: int = Field(..., ge=0, description="Number of chunks retrieved")
    
    # Optional metadata
    metadata: Optional[Dict[str, Any]] = Field(
        None,
        description="Additional metadata about the response"
    )


class HealthResponse(BaseModel):
    """Health check response schema."""
    
    status: str = Field(..., description="Service status")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    version: str = Field(..., description="Application version")
    database_connected: bool = Field(..., description="Database connection status")
    openai_accessible: bool = Field(..., description="OpenAI API accessibility")
    chunks_indexed: int = Field(..., ge=0, description="Number of chunks in database")


class ErrorResponse(BaseModel):
    """Error response schema."""
    
    error: str = Field(..., description="Error message")
    detail: Optional[str] = Field(None, description="Detailed error information")
    error_code: Optional[str] = Field(None, description="Application-specific error code")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    request_id: Optional[str] = Field(None, description="Request tracking ID")


class BulkIngestionRequest(BaseModel):
    """Request schema for bulk document ingestion."""
    
    source_file: str = Field(..., description="Path to source chunks file")
    batch_size: int = Field(default=100, ge=1, le=1000, description="Batch size for processing")
    overwrite_existing: bool = Field(default=False, description="Whether to overwrite existing data")
    validate_embeddings: bool = Field(default=True, description="Whether to validate embedding quality")


class IngestionResponse(BaseModel):
    """Response schema for document ingestion."""
    
    status: str = Field(..., description="Ingestion status")
    chunks_processed: int = Field(..., ge=0, description="Number of chunks processed")
    chunks_inserted: int = Field(..., ge=0, description="Number of chunks successfully inserted")
    chunks_failed: int = Field(..., ge=0, description="Number of chunks that failed")
    processing_time_ms: int = Field(..., ge=0, description="Total processing time")
    errors: List[str] = Field(default_factory=list, description="Any errors encountered")


class SearchRequest(BaseModel):
    """Request schema for semantic search."""
    
    query: str = Field(..., min_length=1, description="Search query")
    max_results: int = Field(default=10, ge=1, le=50, description="Maximum results to return")
    similarity_threshold: float = Field(
        default=0.7,
        ge=0.0,
        le=1.0,
        description="Minimum similarity threshold"
    )
    filter_content_types: Optional[List[ContentType]] = Field(
        None,
        description="Filter by content types"
    )
    filter_sections: Optional[List[str]] = Field(
        None,
        description="Filter by specific sections"
    )


class SearchResult(BaseModel):
    """Individual search result."""
    
    chunk_id: int = Field(..., description="Chunk identifier")
    content: str = Field(..., description="Chunk content")
    similarity_score: float = Field(..., ge=0.0, le=1.0, description="Similarity score")
    metadata: ChunkMetadata = Field(..., description="Chunk metadata")


class SearchResponse(BaseModel):
    """Response schema for semantic search."""
    
    query: str = Field(..., description="Original search query")
    results: List[SearchResult] = Field(default_factory=list, description="Search results")
    total_results: int = Field(..., ge=0, description="Total number of results found")
    search_time_ms: int = Field(..., ge=0, description="Search execution time")
    similarity_threshold_used: float = Field(..., description="Similarity threshold applied")

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/main.py
File type: .py
"""Main entry point for the HIPAA QA System backend."""

import sys
from pathlib import Path

import uvicorn
from loguru import logger

# Add src to path for imports
src_path = Path(__file__).parent.parent
sys.path.insert(0, str(src_path))

from hipaa_qa.api.main import get_app
from hipaa_qa.config import get_settings


def setup_logging() -> None:
    """Configure logging for the application."""
    settings = get_settings()
    
    # Remove default handler
    logger.remove()
    
    # Configure format based on settings
    if settings.log_format == "json":
        format_string = (
            '{{"time": "{time:YYYY-MM-DD HH:mm:ss.SSS}", '
            '"level": "{level}", '
            '"module": "{module}", '
            '"function": "{function}", '
            '"line": {line}, '
            '"message": "{message}"}}'
        )
    else:
        format_string = (
            "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{module}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
            "<level>{message}</level>"
        )
    
    # Add handler with configured format
    logger.add(
        sys.stdout,
        format=format_string,
        level=settings.log_level,
        colorize=settings.log_format == "text",
        serialize=settings.log_format == "json",
    )
    
    # Add file handler for errors
    logger.add(
        "logs/hipaa_qa_errors.log",
        format=format_string,
        level="ERROR",
        rotation="1 day",
        retention="7 days",
        serialize=settings.log_format == "json",
    )
    
    logger.info(f"Logging configured: level={settings.log_level}, format={settings.log_format}")


def main() -> None:
    """Main entry point."""
    # Setup logging first
    setup_logging()
    
    # Get settings
    settings = get_settings()
    
    logger.info("Starting HIPAA QA System backend...")
    logger.info(f"Configuration: host={settings.api_host}, port={settings.api_port}")
    
    # Get FastAPI app
    app = get_app()
    
    # Run with uvicorn
    uvicorn.run(
        app,
        host=settings.api_host,
        port=settings.api_port,
        workers=settings.api_workers,
        log_config=None,  # Use our custom logging
        access_log=False,  # We handle this in middleware
    )


if __name__ == "__main__":
    main()

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/database/models.py
File type: .py
"""SQLAlchemy models for HIPAA QA database tables."""

from datetime import datetime
from typing import List, Optional

from pgvector.sqlalchemy import Vector
from sqlalchemy import (
    JSON,
    Boolean,
    DateTime,
    Float,
    Integer,
    String,
    Text,
    func,
)
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Mapped, mapped_column

# Create declarative base
Base = declarative_base()


class DocumentChunkTable(Base):
    """Database model for storing document chunks with embeddings."""
    
    __tablename__ = "document_chunks"
    
    # Primary key
    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    
    # Chunk identification
    chunk_id: Mapped[int] = mapped_column(Integer, unique=True, nullable=False, index=True)
    
    # Content
    content: Mapped[str] = mapped_column(Text, nullable=False)
    
    # Section information (indexed for fast filtering)
    section_id: Mapped[str] = mapped_column(String(100), nullable=False, index=True)
    section_type: Mapped[str] = mapped_column(String(50), nullable=False, index=True)
    section_title: Mapped[str] = mapped_column(String(500), nullable=False)
    full_reference: Mapped[str] = mapped_column(String(500), nullable=False, index=True)
    cfr_citation: Mapped[Optional[str]] = mapped_column(String(300), nullable=True, index=True)
    parent_section: Mapped[Optional[str]] = mapped_column(String(100), nullable=True, index=True)
    hierarchy_level: Mapped[int] = mapped_column(Integer, nullable=False, index=True)
    
    # Chunk metadata
    chunk_index: Mapped[int] = mapped_column(Integer, nullable=False)
    total_chunks: Mapped[int] = mapped_column(Integer, nullable=False)
    chunk_size: Mapped[int] = mapped_column(Integer, nullable=False)
    word_count: Mapped[int] = mapped_column(Integer, nullable=False)
    
    # Content classification (indexed for filtering)
    contains_definitions: Mapped[bool] = mapped_column(Boolean, default=False, index=True)
    contains_penalties: Mapped[bool] = mapped_column(Boolean, default=False, index=True)
    contains_requirements: Mapped[bool] = mapped_column(Boolean, default=False, index=True)
    
    # Content type and compliance level (indexed for filtering)
    content_type: Mapped[str] = mapped_column(String(50), nullable=False, index=True)
    compliance_level: Mapped[str] = mapped_column(String(50), nullable=False, index=True)
    
    # References and terms (stored as JSON for flexibility)
    references: Mapped[List[str]] = mapped_column("references_json", JSONB, default=list)
    key_terms: Mapped[List[str]] = mapped_column("key_terms_json", JSONB, default=list)
    
    # Vector embedding (pgvector column)
    embedding: Mapped[Optional[List[float]]] = mapped_column(
        Vector(3072),  # text-embedding-3-large dimension
        nullable=True,
        comment="Vector embedding for semantic search"
    )
    
    # Timestamps
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.now(),
        nullable=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.now(),
        onupdate=func.now(),
        nullable=False
    )
    
    # Optional metadata (stored as JSON for extensibility)
    metadata_json: Mapped[Optional[dict]] = mapped_column(
        JSONB,
        nullable=True,
        comment="Additional metadata in JSON format"
    )
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return (
            f"<DocumentChunk(id={self.id}, chunk_id={self.chunk_id}, "
            f"section_id='{self.section_id}', content_length={len(self.content)})>"
        )
        
    def to_dict(self) -> dict:
        """Convert model to dictionary representation."""
        return {
            "id": self.id,
            "chunk_id": self.chunk_id,
            "content": self.content,
            "section_id": self.section_id,
            "section_type": self.section_type,
            "section_title": self.section_title,
            "full_reference": self.full_reference,
            "cfr_citation": self.cfr_citation,
            "parent_section": self.parent_section,
            "hierarchy_level": self.hierarchy_level,
            "chunk_index": self.chunk_index,
            "total_chunks": self.total_chunks,
            "chunk_size": self.chunk_size,
            "word_count": self.word_count,
            "contains_definitions": self.contains_definitions,
            "contains_penalties": self.contains_penalties,
            "contains_requirements": self.contains_requirements,
            "content_type": self.content_type,
            "compliance_level": self.compliance_level,
            "references": self.references,
            "key_terms": self.key_terms,
            "created_at": self.created_at,
            "updated_at": self.updated_at,
            "metadata_json": self.metadata_json,
        }


class QueryLog(Base):
    """Model for logging user queries and responses for analytics."""
    
    __tablename__ = "query_logs"
    
    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    
    # Query information
    question: Mapped[str] = mapped_column(Text, nullable=False)
    answer: Mapped[str] = mapped_column(Text, nullable=False)
    
    # Performance metrics
    processing_time_ms: Mapped[int] = mapped_column(Integer, nullable=False)
    chunks_retrieved: Mapped[int] = mapped_column(Integer, nullable=False)
    
    # Quality metrics
    confidence_score: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    similarity_threshold_used: Mapped[float] = mapped_column(Float, nullable=False)
    
    # Model information
    embedding_model: Mapped[str] = mapped_column(String(100), nullable=False)
    chat_model: Mapped[str] = mapped_column(String(100), nullable=False)
    
    # Retrieved chunk IDs (for analysis)
    retrieved_chunk_ids: Mapped[List[int]] = mapped_column(JSONB, default=list)
    
    # User session information (optional)
    session_id: Mapped[Optional[str]] = mapped_column(String(100), nullable=True, index=True)
    user_ip: Mapped[Optional[str]] = mapped_column(String(45), nullable=True)  # IPv6 compatible
    
    # Timestamp
    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        server_default=func.now(),
        nullable=False,
        index=True
    )
    
    def __repr__(self) -> str:
        """String representation of the model."""
        return (
            f"<QueryLog(id={self.id}, question_length={len(self.question)}, "
            f"processing_time={self.processing_time_ms}ms)>"
        )

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/database/__init__.py
File type: .py
"""Database module for HIPAA QA System."""

from .connection import DatabaseManager, get_database, wait_for_database
from .models import DocumentChunkTable
from .repository import ChunkRepository

__all__ = [
    "DatabaseManager",
    "get_database", 
    "wait_for_database",
    "DocumentChunkTable",
    "ChunkRepository",
]

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/database/connection.py
File type: .py
"""Database connection management for PostgreSQL with pgvector."""

import asyncio
from contextlib import asynccontextmanager
from typing import AsyncGenerator, Optional

import asyncpg
from loguru import logger
from sqlalchemy.ext.asyncio import (
    AsyncEngine,
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)
from sqlalchemy.pool import NullPool

from ..config import Settings, get_settings


class DatabaseManager:
    """Manages database connections and operations."""
    
    def __init__(self, settings: Optional[Settings] = None) -> None:
        """Initialize database manager."""
        self.settings = settings or get_settings()
        self._engine: Optional[AsyncEngine] = None
        self._session_factory: Optional[async_sessionmaker[AsyncSession]] = None
        self._connection_pool: Optional[asyncpg.Pool] = None
        
    @property
    def engine(self) -> AsyncEngine:
        """Get or create the SQLAlchemy async engine."""
        if self._engine is None:
            self._engine = create_async_engine(
                self.settings.database_url,
                echo=self.settings.api_debug,
                poolclass=NullPool,  # Use asyncpg pool instead
                pool_pre_ping=True,
                future=True,
            )
            logger.info("Created SQLAlchemy async engine")
        return self._engine
        
    @property
    def session_factory(self) -> async_sessionmaker[AsyncSession]:
        """Get or create the session factory."""
        if self._session_factory is None:
            self._session_factory = async_sessionmaker(
                bind=self.engine,
                class_=AsyncSession,
                expire_on_commit=False,
            )
            logger.info("Created SQLAlchemy session factory")
        return self._session_factory
        
    async def get_connection_pool(self) -> asyncpg.Pool:
        """Get or create the asyncpg connection pool."""
        if self._connection_pool is None:
            self._connection_pool = await asyncpg.create_pool(
                host=self.settings.db_host,
                port=self.settings.db_port,
                user=self.settings.db_user,
                password=self.settings.db_password,
                database=self.settings.db_name,
                min_size=1,
                max_size=self.settings.db_pool_size,
                command_timeout=60,
                server_settings={
                    'application_name': 'hipaa_qa_system',
                },
            )
            logger.info(
                f"Created asyncpg connection pool "
                f"(size: {self.settings.db_pool_size})"
            )
        return self._connection_pool
        
    async def initialize_database(self) -> None:
        """Initialize database with pgvector extension and tables."""
        logger.info("Initializing database...")
        
        try:
            pool = await self.get_connection_pool()
            async with pool.acquire() as conn:
                # Enable pgvector extension
                await conn.execute("CREATE EXTENSION IF NOT EXISTS vector")
                logger.info("Enabled pgvector extension")
                
                # Import and create tables
                from .models import Base
                async with self.engine.begin() as conn:
                    await conn.run_sync(Base.metadata.create_all)
                logger.info("Created database tables")
                
        except Exception as e:
            logger.error(f"Failed to initialize database: {e}")
            raise
            
    async def check_connection(self) -> bool:
        """Check if database connection is healthy."""
        try:
            pool = await self.get_connection_pool()
            async with pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            return True
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
            return False
            
    async def get_chunks_count(self) -> int:
        """Get the total number of document chunks in the database."""
        try:
            pool = await self.get_connection_pool()
            async with pool.acquire() as conn:
                count = await conn.fetchval(
                    "SELECT COUNT(*) FROM document_chunks"
                )
                return count or 0
        except Exception as e:
            logger.error(f"Failed to get chunks count: {e}")
            return 0
            
    @asynccontextmanager
    async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
        """Get an async database session."""
        async with self.session_factory() as session:
            try:
                yield session
            except Exception:
                await session.rollback()
                raise
            finally:
                await session.close()
                
    @asynccontextmanager
    async def get_connection(self) -> AsyncGenerator[asyncpg.Connection, None]:
        """Get a raw asyncpg connection for vector operations."""
        pool = await self.get_connection_pool()
        async with pool.acquire() as conn:
            yield conn
            
    async def close(self) -> None:
        """Close all database connections."""
        if self._connection_pool:
            await self._connection_pool.close()
            logger.info("Closed asyncpg connection pool")
            
        if self._engine:
            await self._engine.dispose()
            logger.info("Disposed SQLAlchemy engine")


# Global database manager instance
_db_manager: Optional[DatabaseManager] = None


def get_database() -> DatabaseManager:
    """Get the global database manager instance."""
    global _db_manager
    if _db_manager is None:
        _db_manager = DatabaseManager()
    return _db_manager


async def wait_for_database(
    max_retries: int = 30,
    retry_interval: float = 1.0,
    settings: Optional[Settings] = None,
) -> bool:
    """Wait for database to become available."""
    settings = settings or get_settings()
    
    for attempt in range(max_retries):
        try:
            # Try to connect using asyncpg directly
            conn = await asyncpg.connect(
                host=settings.db_host,
                port=settings.db_port,
                user=settings.db_user,
                password=settings.db_password,
                database=settings.db_name,
                timeout=5.0,
            )
            await conn.fetchval("SELECT 1")
            await conn.close()
            logger.info(f"Database connection successful after {attempt + 1} attempts")
            return True
            
        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(
                    f"Database connection attempt {attempt + 1} failed: {e}. "
                    f"Retrying in {retry_interval}s..."
                )
                await asyncio.sleep(retry_interval)
            else:
                logger.error(f"Database connection failed after {max_retries} attempts: {e}")
                return False
                
    return False

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/database/repository.py
File type: .py
"""Repository pattern for database operations."""

import json
from typing import Any, Dict, List, Optional, Tuple

import asyncpg
from loguru import logger
from sqlalchemy import delete, select, text
from sqlalchemy.ext.asyncio import AsyncSession

from ..schemas import ChunkMetadata, ContentType, DocumentChunk
from .connection import DatabaseManager
from .models import DocumentChunkTable


class ChunkRepository:
    """Repository for document chunk operations."""
    
    def __init__(self, db_manager: DatabaseManager) -> None:
        """Initialize repository with database manager."""
        self.db_manager = db_manager
        
    async def insert_chunk(
        self, 
        chunk: DocumentChunk,
        session: Optional[AsyncSession] = None
    ) -> int:
        """Insert a single document chunk."""
        chunk_data = DocumentChunkTable(
            chunk_id=chunk.chunk_id,
            content=chunk.content,
            section_id=chunk.metadata.section_id,
            section_type=chunk.metadata.section_type.value,
            section_title=chunk.metadata.section_title,
            full_reference=chunk.metadata.full_reference,
            cfr_citation=chunk.metadata.cfr_citation,
            parent_section=chunk.metadata.parent_section,
            hierarchy_level=chunk.metadata.hierarchy_level,
            chunk_index=chunk.metadata.chunk_index,
            total_chunks=chunk.metadata.total_chunks,
            chunk_size=chunk.metadata.chunk_size,
            word_count=chunk.metadata.word_count,
            contains_definitions=chunk.metadata.contains_definitions,
            contains_penalties=chunk.metadata.contains_penalties,
            contains_requirements=chunk.metadata.contains_requirements,
            content_type=chunk.metadata.content_type.value,
            compliance_level=chunk.metadata.compliance_level.value,
            references=chunk.metadata.references,
            key_terms=chunk.metadata.key_terms,
            embedding=chunk.embedding,
            created_at=chunk.created_at,
            updated_at=chunk.updated_at,
        )
        
        if session:
            session.add(chunk_data)
            await session.flush()
            return chunk_data.id
        else:
            async with self.db_manager.get_session() as session:
                session.add(chunk_data)
                await session.commit()
                return chunk_data.id
                
    async def bulk_insert_chunks(
        self, 
        chunks: List[DocumentChunk],
        batch_size: int = 100
    ) -> Tuple[int, int]:
        """
        Bulk insert document chunks with embeddings.
        Returns (success_count, error_count).
        """
        success_count = 0
        error_count = 0
        
        logger.info(f"Starting bulk insert of {len(chunks)} chunks...")
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            
            try:
                async with self.db_manager.get_connection() as conn:
                    # Prepare data for insertion
                    chunk_data = []
                    for chunk in batch:
                        chunk_data.append((
                            chunk.chunk_id,
                            chunk.content,
                            chunk.metadata.section_id,
                            chunk.metadata.section_type.value,
                            chunk.metadata.section_title,
                            chunk.metadata.full_reference,
                            chunk.metadata.cfr_citation,
                            chunk.metadata.parent_section,
                            chunk.metadata.hierarchy_level,
                            chunk.metadata.chunk_index,
                            chunk.metadata.total_chunks,
                            chunk.metadata.chunk_size,
                            chunk.metadata.word_count,
                            chunk.metadata.contains_definitions,
                            chunk.metadata.contains_penalties,
                            chunk.metadata.contains_requirements,
                            chunk.metadata.content_type.value,
                            chunk.metadata.compliance_level.value,
                            json.dumps(chunk.metadata.references),
                            json.dumps(chunk.metadata.key_terms),
                            f"[{','.join(map(str, chunk.embedding))}]" if chunk.embedding else None,  # Format for pgvector
                            chunk.created_at,
                            chunk.updated_at,
                        ))
                    
                    # Execute bulk insert
                    await conn.executemany(
                        """
                        INSERT INTO document_chunks (
                            chunk_id, content, section_id, section_type, section_title,
                            full_reference, cfr_citation, parent_section, hierarchy_level,
                            chunk_index, total_chunks, chunk_size, word_count,
                            contains_definitions, contains_penalties, contains_requirements,
                            content_type, compliance_level, references_json, key_terms_json,
                            embedding, created_at, updated_at
                        ) VALUES (
                            $1, $2, $3, $4, $5, $6, $7, $8, $9, $10,
                            $11, $12, $13, $14, $15, $16, $17, $18, $19, $20,
                            $21, $22, $23
                        )
                        ON CONFLICT (chunk_id) DO UPDATE SET
                            content = EXCLUDED.content,
                            embedding = EXCLUDED.embedding,
                            updated_at = EXCLUDED.updated_at
                        """,
                        chunk_data
                    )
                    
                success_count += len(batch)
                logger.info(f"Inserted batch {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1}")
                
            except Exception as e:
                error_count += len(batch)
                logger.error(f"Failed to insert batch {i//batch_size + 1}: {e}")
                
        logger.info(f"Bulk insert completed: {success_count} success, {error_count} errors")
        return success_count, error_count
        
    async def similarity_search(
        self,
        query_embedding: List[float],
        limit: int = 5,
        similarity_threshold: float = 0.7,
        content_types: Optional[List[ContentType]] = None,
        sections: Optional[List[str]] = None,
    ) -> List[Tuple[DocumentChunkTable, float]]:
        """
        Perform similarity search using pgvector.
        Returns list of (chunk, similarity_score) tuples.
        """
        async with self.db_manager.get_connection() as conn:
            # Build query conditions
            where_conditions = ["embedding IS NOT NULL"]
            params = [query_embedding, limit]
            param_count = 2
            
            if content_types:
                param_count += 1
                where_conditions.append(f"content_type = ANY(${param_count})")
                params.append([ct.value for ct in content_types])
                
            if sections:
                param_count += 1
                where_conditions.append(f"section_id = ANY(${param_count})")
                params.append(sections)
            
            where_clause = " AND ".join(where_conditions)
            
            # Construct similarity search query
            query = f"""
                SELECT 
                    id, chunk_id, content, section_id, section_type, section_title,
                    full_reference, cfr_citation, parent_section, hierarchy_level,
                    chunk_index, total_chunks, chunk_size, word_count,
                    contains_definitions, contains_penalties, contains_requirements,
                    content_type, compliance_level, references_json, key_terms_json,
                    created_at, updated_at, metadata_json,
                    1 - (embedding <=> $1) as similarity_score
                FROM document_chunks
                WHERE {where_clause}
                ORDER BY embedding <=> $1
                LIMIT $2
            """
            
            # Format query vector for pgvector
            query_vector_str = f"[{','.join(map(str, query_embedding))}]"
            formatted_params = [query_vector_str] + params[1:]
            rows = await conn.fetch(query, *formatted_params)
            
            # Filter by similarity threshold and convert to objects
            results = []
            for row in rows:
                similarity_score = row['similarity_score']
                if similarity_score >= similarity_threshold:
                    # Convert row to DocumentChunkTable object
                    chunk = DocumentChunkTable(
                        id=row['id'],
                        chunk_id=row['chunk_id'],
                        content=row['content'],
                        section_id=row['section_id'],
                        section_type=row['section_type'],
                        section_title=row['section_title'],
                        full_reference=row['full_reference'],
                        cfr_citation=row['cfr_citation'],
                        parent_section=row['parent_section'],
                        hierarchy_level=row['hierarchy_level'],
                        chunk_index=row['chunk_index'],
                        total_chunks=row['total_chunks'],
                        chunk_size=row['chunk_size'],
                        word_count=row['word_count'],
                        contains_definitions=row['contains_definitions'],
                        contains_penalties=row['contains_penalties'],
                        contains_requirements=row['contains_requirements'],
                        content_type=row['content_type'],
                        compliance_level=row['compliance_level'],
                        references=row['references_json'] or [],
                        key_terms=row['key_terms_json'] or [],
                        created_at=row['created_at'],
                        updated_at=row['updated_at'],
                        metadata_json=row['metadata_json'],
                    )
                    results.append((chunk, similarity_score))
                    
            logger.info(
                f"Similarity search returned {len(results)} chunks "
                f"above threshold {similarity_threshold}"
            )
            return results
            
    async def get_chunk_by_id(self, chunk_id: int) -> Optional[DocumentChunkTable]:
        """Get a chunk by its chunk_id."""
        async with self.db_manager.get_session() as session:
            stmt = select(DocumentChunkTable).where(
                DocumentChunkTable.chunk_id == chunk_id
            )
            result = await session.execute(stmt)
            return result.scalar_one_or_none()
            
    async def get_chunks_by_section(
        self, 
        section_id: str
    ) -> List[DocumentChunkTable]:
        """Get all chunks for a specific section."""
        async with self.db_manager.get_session() as session:
            stmt = select(DocumentChunkTable).where(
                DocumentChunkTable.section_id == section_id
            ).order_by(DocumentChunkTable.chunk_index)
            result = await session.execute(stmt)
            return list(result.scalars().all())
            
    async def count_chunks(self) -> int:
        """Get total number of chunks in database."""
        async with self.db_manager.get_session() as session:
            stmt = select(text("COUNT(*)")).select_from(DocumentChunkTable)
            result = await session.execute(stmt)
            return result.scalar() or 0
            
    async def count_chunks_with_embeddings(self) -> int:
        """Get number of chunks that have embeddings."""
        async with self.db_manager.get_session() as session:
            stmt = select(text("COUNT(*)")).select_from(DocumentChunkTable).where(
                DocumentChunkTable.embedding.is_not(None)
            )
            result = await session.execute(stmt)
            return result.scalar() or 0
            
    async def delete_all_chunks(self) -> int:
        """Delete all chunks from database. Returns number of deleted rows."""
        async with self.db_manager.get_session() as session:
            stmt = delete(DocumentChunkTable)
            result = await session.execute(stmt)
            await session.commit()
            deleted_count = result.rowcount or 0
            logger.info(f"Deleted {deleted_count} chunks from database")
            return deleted_count
            
    async def get_sections_summary(self) -> Dict[str, Any]:
        """Get summary statistics about sections in the database."""
        async with self.db_manager.get_connection() as conn:
            query = """
                SELECT 
                    section_type,
                    COUNT(*) as chunk_count,
                    COUNT(CASE WHEN embedding IS NOT NULL THEN 1 END) as embedded_count,
                    COUNT(CASE WHEN contains_definitions THEN 1 END) as definition_count,
                    COUNT(CASE WHEN contains_requirements THEN 1 END) as requirement_count,
                    COUNT(CASE WHEN contains_penalties THEN 1 END) as penalty_count
                FROM document_chunks
                GROUP BY section_type
                ORDER BY section_type
            """
            
            rows = await conn.fetch(query)
            
            summary = {
                "by_section_type": {},
                "total_chunks": 0,
                "total_embedded": 0,
            }
            
            for row in rows:
                section_type = row['section_type']
                summary["by_section_type"][section_type] = {
                    "chunk_count": row['chunk_count'],
                    "embedded_count": row['embedded_count'],
                    "definition_count": row['definition_count'],
                    "requirement_count": row['requirement_count'],
                    "penalty_count": row['penalty_count'],
                }
                summary["total_chunks"] += row['chunk_count']
                summary["total_embedded"] += row['embedded_count']
                
            return summary

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/__init__.py
File type: .py
"""API module for HIPAA QA System."""

from .main import create_app, get_app
from .routes import health, ingestion, qa

__all__ = [
    "create_app",
    "get_app",
    "health",
    "ingestion", 
    "qa",
]

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/main.py
File type: .py
"""FastAPI application factory and setup."""

from contextlib import asynccontextmanager
from typing import AsyncGenerator

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from loguru import logger

from ..config import get_settings
from .middleware import LoggingMiddleware, RequestIDMiddleware
from .routes import health, ingestion, qa
from .state import get_app_state


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[None, None]:
    """Application lifespan context manager."""
    logger.info("Starting HIPAA QA System...")
    
    try:
        # Get app state
        app_state = get_app_state()
        
        # Initialize database
        await app_state.db_manager.initialize_database()
        logger.info("Database initialized successfully")
        
        # Validate API access
        embedding_valid = await app_state.embedding_service.validate_api_access()
        chat_valid = await app_state.qa_service.validate_chat_api_access()
        
        if not embedding_valid:
            logger.warning("OpenAI Embedding API validation failed")
        if not chat_valid:
            logger.warning("OpenAI Chat API validation failed")
            
        # Log ingestion status
        status = await app_state.ingestion_service.get_ingestion_status()
        logger.info(f"Ingestion status: {status}")
        
        logger.info("HIPAA QA System startup complete")
        
        # Application is ready
        yield
        
    except Exception as e:
        logger.error(f"Startup failed: {e}")
        raise
        
    finally:
        # Cleanup
        logger.info("Shutting down HIPAA QA System...")
        app_state = get_app_state()
        await app_state.db_manager.close()
        logger.info("Shutdown complete")


def create_app() -> FastAPI:
    """Create and configure FastAPI application."""
    settings = get_settings()
    
    app = FastAPI(
        title="HIPAA QA System",
        description="RAG-based Question Answering System for HIPAA Regulations",
        version="0.1.0",
        docs_url="/docs",
        redoc_url="/redoc",
        openapi_url="/openapi.json",
        lifespan=lifespan,
    )
    
    # Add middleware
    app.add_middleware(
        CORSMiddleware,
        allow_origins=settings.api_cors_origins,
        allow_credentials=True,
        allow_methods=["GET", "POST", "PUT", "DELETE"],
        allow_headers=["*"],
    )
    
    app.add_middleware(RequestIDMiddleware)
    app.add_middleware(LoggingMiddleware)
    
    # Add exception handlers
    @app.exception_handler(Exception)
    async def global_exception_handler(request, exc):
        logger.error(f"Unhandled exception: {exc}", exc_info=True)
        return JSONResponse(
            status_code=500,
            content={
                "error": "Internal server error",
                "detail": str(exc) if settings.api_debug else "An unexpected error occurred",
            }
        )
    
    # Include routers
    app.include_router(health.router, prefix="/health", tags=["Health"])
    app.include_router(qa.router, prefix="/qa", tags=["Question Answering"])
    app.include_router(ingestion.router, prefix="/ingestion", tags=["Data Ingestion"])
    
    # Root endpoint
    @app.get("/", summary="Root endpoint")
    async def root():
        """Root endpoint with basic API information."""
        return {
            "service": "HIPAA QA System",
            "version": "0.1.0",
            "description": "RAG-based Question Answering for HIPAA Regulations",
            "docs": "/docs",
            "health": "/health",
        }
    
    return app


# Global app instance
_app: FastAPI = None


def get_app() -> FastAPI:
    """Get the global FastAPI application instance."""
    global _app
    if _app is None:
        _app = create_app()
    return _app




--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/middleware.py
File type: .py
"""Custom middleware for the FastAPI application."""

import time
import uuid
from typing import Callable

from fastapi import Request, Response
from loguru import logger
from starlette.middleware.base import BaseHTTPMiddleware


class RequestIDMiddleware(BaseHTTPMiddleware):
    """Middleware to add unique request IDs to all requests."""
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Add request ID to request and response headers."""
        request_id = str(uuid.uuid4())
        
        # Add to request state
        request.state.request_id = request_id
        
        # Process request
        response = await call_next(request)
        
        # Add to response headers
        response.headers["X-Request-ID"] = request_id
        
        return response


class LoggingMiddleware(BaseHTTPMiddleware):
    """Middleware for request/response logging."""
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        """Log request and response details."""
        start_time = time.time()
        
        # Get request ID
        request_id = getattr(request.state, "request_id", "unknown")
        
        # Log request
        logger.info(
            f"Request started",
            extra={
                "request_id": request_id,
                "method": request.method,
                "url": str(request.url),
                "client_ip": request.client.host if request.client else "unknown",
                "user_agent": request.headers.get("user-agent", "unknown"),
            }
        )
        
        try:
            # Process request
            response = await call_next(request)
            
            # Calculate processing time
            process_time = time.time() - start_time
            
            # Log response
            logger.info(
                f"Request completed",
                extra={
                    "request_id": request_id,
                    "status_code": response.status_code,
                    "process_time_ms": round(process_time * 1000, 2),
                }
            )
            
            # Add processing time to headers
            response.headers["X-Process-Time"] = str(round(process_time * 1000, 2))
            
            return response
            
        except Exception as e:
            # Calculate processing time for error case
            process_time = time.time() - start_time
            
            # Log error
            logger.error(
                f"Request failed",
                extra={
                    "request_id": request_id,
                    "error": str(e),
                    "process_time_ms": round(process_time * 1000, 2),
                },
                exc_info=True
            )
            
            # Re-raise the exception
            raise

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/dependencies.py
File type: .py
"""FastAPI dependency injection functions."""

from .state import AppState, get_app_state as _get_app_state


def get_app_state() -> AppState:
    """Dependency to get the application state."""
    return _get_app_state()

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/state.py
File type: .py
"""Application state management."""

from ..config import Settings, get_settings
from ..database import DatabaseManager, get_database
from ..services import EmbeddingService, IngestionService, QAService


class AppState:
    """Application state container."""
    
    def __init__(self) -> None:
        self.settings: Settings = get_settings()
        self.db_manager: DatabaseManager = get_database()
        self.embedding_service: EmbeddingService = EmbeddingService(self.settings)
        self.ingestion_service: IngestionService = IngestionService(
            self.db_manager, self.embedding_service
        )
        self.qa_service: QAService = QAService(
            self.db_manager, self.embedding_service, self.settings
        )


# Global app state
_app_state: AppState = None


def get_app_state() -> AppState:
    """Get the global application state."""
    global _app_state
    if _app_state is None:
        _app_state = AppState()
    return _app_state

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/routes/ingestion.py
File type: .py
"""Data ingestion API endpoints."""

from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from loguru import logger

from ...schemas import BulkIngestionRequest, IngestionResponse
from ..dependencies import get_app_state
from ..state import AppState

router = APIRouter()


@router.post("/ingest", response_model=IngestionResponse, summary="Ingest document chunks")
async def ingest_chunks(
    request: BulkIngestionRequest,
    background_tasks: BackgroundTasks,
    app_state: AppState = Depends(get_app_state)
):
    """
    Ingest document chunks from JSON file into the database.
    
    This endpoint processes the HIPAA regulation chunks, generates embeddings,
    and stores them in the vector database for semantic search.
    
    Args:
        request: Ingestion request with file path and options
        background_tasks: FastAPI background tasks for async processing
        
    Returns:
        IngestionResponse: Processing status and statistics
        
    Raises:
        HTTPException: 400 for invalid requests, 500 for processing errors
    """
    try:
        import time
        start_time = time.time()
        
        logger.info(f"Starting ingestion from {request.source_file}")
        
        # Validate source file path
        if not request.source_file:
            raise HTTPException(
                status_code=400,
                detail="Source file path is required"
            )
            
        # Perform ingestion
        success_count, error_count, errors = await app_state.ingestion_service.ingest_from_json(
            json_file_path=request.source_file,
            batch_size=request.batch_size,
            overwrite_existing=request.overwrite_existing,
            generate_embeddings=True,  # Always generate embeddings for new ingestion
        )
        
        # Calculate processing time
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        # Determine status
        if error_count == 0:
            status = "completed"
        elif success_count > error_count:
            status = "completed_with_errors"
        else:
            status = "failed"
            
        response = IngestionResponse(
            status=status,
            chunks_processed=success_count + error_count,
            chunks_inserted=success_count,
            chunks_failed=error_count,
            processing_time_ms=processing_time_ms,
            errors=errors[:10],  # Limit to first 10 errors
        )
        
        logger.info(
            f"Ingestion completed: {success_count} success, {error_count} errors, "
            f"{processing_time_ms}ms"
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error during ingestion: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Ingestion failed: {str(e)}"
        )


@router.post("/recompute-embeddings", summary="Recompute embeddings for existing chunks")
async def recompute_embeddings(
    section_filter: str = None,
    batch_size: int = 100,
    app_state: AppState = Depends(get_app_state)
):
    """
    Recompute embeddings for existing chunks.
    
    Useful when switching embedding models or fixing embedding issues.
    
    Args:
        section_filter: Optional section ID to filter chunks
        batch_size: Batch size for processing
        
    Returns:
        dict: Recomputation results
        
    Raises:
        HTTPException: 500 for processing errors
    """
    try:
        import time
        start_time = time.time()
        
        logger.info(f"Starting embedding recomputation (section: {section_filter})")
        
        success_count, error_count = await app_state.ingestion_service.recompute_embeddings(
            batch_size=batch_size,
            section_filter=section_filter,
        )
        
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        logger.info(
            f"Embedding recomputation completed: {success_count} success, "
            f"{error_count} errors, {processing_time_ms}ms"
        )
        
        return {
            "status": "completed" if error_count == 0 else "completed_with_errors",
            "chunks_recomputed": success_count,
            "chunks_failed": error_count,
            "processing_time_ms": processing_time_ms,
            "section_filter": section_filter,
        }
        
    except Exception as e:
        logger.error(f"Error during embedding recomputation: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Embedding recomputation failed: {str(e)}"
        )


@router.get("/status", summary="Get ingestion status")
async def get_ingestion_status(app_state: AppState = Depends(get_app_state)):
    """
    Get current ingestion status and statistics.
    
    Returns:
        dict: Ingestion status information
    """
    try:
        status = await app_state.ingestion_service.get_ingestion_status()
        return status
        
    except Exception as e:
        logger.error(f"Error getting ingestion status: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get ingestion status: {str(e)}"
        )


@router.delete("/clear", summary="Clear all ingested data")
async def clear_ingested_data(
    confirm: bool = False,
    app_state: AppState = Depends(get_app_state)
):
    """
    Clear all ingested document chunks from the database.
    
    ‚ö†Ô∏è WARNING: This operation is destructive and cannot be undone!
    
    Args:
        confirm: Must be True to proceed with deletion
        
    Returns:
        dict: Deletion results
        
    Raises:
        HTTPException: 400 if not confirmed, 500 for processing errors
    """
    if not confirm:
        raise HTTPException(
            status_code=400,
            detail="Must set confirm=true to clear all data. This operation cannot be undone."
        )
        
    try:
        logger.warning("Clearing all ingested data...")
        
        deleted_count = await app_state.ingestion_service.repository.delete_all_chunks()
        
        logger.warning(f"Cleared {deleted_count} chunks from database")
        
        return {
            "status": "completed",
            "chunks_deleted": deleted_count,
            "message": "All ingested data has been cleared"
        }
        
    except Exception as e:
        logger.error(f"Error clearing data: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to clear data: {str(e)}"
        )

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/routes/health.py
File type: .py
"""Health check endpoints."""

from datetime import datetime

from fastapi import APIRouter, Depends, HTTPException
from loguru import logger

from ...schemas import HealthResponse
from ..dependencies import get_app_state
from ..state import AppState

router = APIRouter()


@router.get("/", response_model=HealthResponse, summary="Basic health check")
async def health_check(app_state: AppState = Depends(get_app_state)):
    """
    Basic health check endpoint.
    
    Returns:
        HealthResponse: Current service health status
    """
    try:
        # Check database connection
        db_connected = await app_state.db_manager.check_connection()
        
        # Check OpenAI API access
        openai_accessible = await app_state.embedding_service.validate_api_access()
        
        # Get chunk count
        chunks_indexed = await app_state.db_manager.get_chunks_count()
        
        # Determine overall status
        if db_connected and openai_accessible and chunks_indexed > 0:
            status = "healthy"
        elif db_connected and chunks_indexed > 0:
            status = "degraded"  # API issues but core functionality works
        else:
            status = "unhealthy"
            
        return HealthResponse(
            status=status,
            timestamp=datetime.utcnow(),
            version="0.1.0",
            database_connected=db_connected,
            openai_accessible=openai_accessible,
            chunks_indexed=chunks_indexed,
        )
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return HealthResponse(
            status="unhealthy",
            timestamp=datetime.utcnow(),
            version="0.1.0",
            database_connected=False,
            openai_accessible=False,
            chunks_indexed=0,
        )


@router.get("/detailed", summary="Detailed health information")
async def detailed_health(app_state: AppState = Depends(get_app_state)):
    """
    Detailed health information including database and service statistics.
    
    Returns:
        dict: Detailed health and status information
    """
    try:
        # Basic health check
        basic_health = await health_check(app_state)
        
        # Additional detailed information
        ingestion_status = await app_state.ingestion_service.get_ingestion_status()
        sections_summary = await app_state.ingestion_service.repository.get_sections_summary()
        
        # Embedding model info
        embedding_info = app_state.embedding_service.get_model_info()
        
        return {
            "basic_health": basic_health.dict(),
            "ingestion_status": ingestion_status,
            "sections_summary": sections_summary,
            "embedding_model": embedding_info,
            "settings": {
                "embedding_dimension": app_state.settings.embedding_dimension,
                "similarity_threshold": app_state.settings.similarity_threshold,
                "max_chunks_retrieved": app_state.settings.max_chunks_retrieved,
                "chat_model": app_state.settings.openai_chat_model,
            }
        }
        
    except Exception as e:
        logger.error(f"Detailed health check failed: {e}")
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")


@router.get("/ready", summary="Readiness probe")
async def readiness_check(app_state: AppState = Depends(get_app_state)):
    """
    Kubernetes-style readiness probe.
    
    Returns:
        dict: Simple ready/not ready status
        
    Raises:
        HTTPException: 503 if service is not ready
    """
    try:
        # Check if service is ready to handle requests
        db_connected = await app_state.db_manager.check_connection()
        chunks_count = await app_state.db_manager.get_chunks_count()
        
        if db_connected and chunks_count > 0:
            return {"status": "ready"}
        else:
            raise HTTPException(
                status_code=503,
                detail="Service not ready - database not connected or no data loaded"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Readiness check failed: {e}")
        raise HTTPException(
            status_code=503,
            detail=f"Service not ready: {str(e)}"
        )


@router.get("/live", summary="Liveness probe")
async def liveness_check():
    """
    Kubernetes-style liveness probe.
    
    Returns:
        dict: Simple alive status
    """
    return {"status": "alive", "timestamp": datetime.utcnow()}

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/routes/__init__.py
File type: .py
"""API routes module."""

from . import health, ingestion, qa

__all__ = ["health", "ingestion", "qa"]

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/api/routes/qa.py
File type: .py
"""Question-answering API endpoints."""

from typing import List, Optional

from fastapi import APIRouter, Depends, HTTPException
from loguru import logger

from ...schemas import AnswerResponse, ContentType, QuestionRequest, SearchRequest, SearchResponse
from ..dependencies import get_app_state
from ..state import AppState

router = APIRouter()


@router.post("/ask", response_model=AnswerResponse, summary="Ask a question about HIPAA")
async def ask_question(
    request: QuestionRequest,
    app_state: AppState = Depends(get_app_state)
):
    """
    Ask a question about HIPAA regulations and get an answer with citations.
    
    This endpoint uses a RAG (Retrieval-Augmented Generation) pipeline to:
    1. Find relevant regulation text chunks using semantic similarity
    2. Generate an answer using OpenAI's GPT model with proper citations
    3. Return the answer along with source references
    
    Args:
        request: Question request with the user's question and optional parameters
        
    Returns:
        AnswerResponse: Generated answer with citations and source references
        
    Raises:
        HTTPException: 400 for invalid requests, 500 for processing errors
    """
    try:
        logger.info(f"Processing question: {request.question[:100]}...")
        
        # Validate request
        if not request.question.strip():
            raise HTTPException(
                status_code=400,
                detail="Question cannot be empty"
            )
            
        # Process the question using QA service
        response = await app_state.qa_service.answer_question(
            question=request.question,
            max_chunks=request.max_chunks or app_state.settings.max_chunks_retrieved,
            similarity_threshold=request.similarity_threshold or app_state.settings.similarity_threshold,
        )
        
        logger.info(
            f"Question processed successfully: "
            f"{response.chunks_retrieved} chunks, "
            f"{response.processing_time_ms}ms, "
            f"confidence: {response.confidence_score}"
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing question: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process question: {str(e)}"
        )


@router.post("/search", response_model=SearchResponse, summary="Search regulation chunks")
async def search_chunks(
    request: SearchRequest,
    app_state: AppState = Depends(get_app_state)
):
    """
    Search for regulation chunks using semantic similarity.
    
    This endpoint performs semantic search without generating an answer,
    useful for exploring the knowledge base or debugging retrieval.
    
    Args:
        request: Search request with query and filters
        
    Returns:
        SearchResponse: List of matching chunks with similarity scores
        
    Raises:
        HTTPException: 400 for invalid requests, 500 for processing errors
    """
    try:
        import time
        start_time = time.time()
        
        logger.info(f"Searching for: {request.query[:100]}...")
        
        # Generate query embedding
        query_embedding = await app_state.embedding_service.embed_text(request.query)
        
        # Perform similarity search
        chunk_results = await app_state.qa_service.repository.similarity_search(
            query_embedding=query_embedding,
            limit=request.max_results,
            similarity_threshold=request.similarity_threshold,
            content_types=request.filter_content_types,
            sections=request.filter_sections,
        )
        
        # Build search results
        from ...schemas import SearchResult, ChunkMetadata, SectionType, ContentType, ComplianceLevel
        
        results = []
        for chunk, similarity_score in chunk_results:
            # Convert database model to schema
            metadata = ChunkMetadata(
                section_id=chunk.section_id,
                section_type=SectionType(chunk.section_type),
                section_title=chunk.section_title,
                full_reference=chunk.full_reference,
                cfr_citation=chunk.cfr_citation,
                parent_section=chunk.parent_section,
                hierarchy_level=chunk.hierarchy_level,
                chunk_index=chunk.chunk_index,
                total_chunks=chunk.total_chunks,
                chunk_size=chunk.chunk_size,
                word_count=chunk.word_count,
                contains_definitions=chunk.contains_definitions,
                contains_penalties=chunk.contains_penalties,
                contains_requirements=chunk.contains_requirements,
                references=chunk.references or [],
                key_terms=chunk.key_terms or [],
                content_type=ContentType(chunk.content_type),
                compliance_level=ComplianceLevel(chunk.compliance_level),
            )
            
            result = SearchResult(
                chunk_id=chunk.chunk_id,
                content=chunk.content,
                similarity_score=similarity_score,
                metadata=metadata,
            )
            results.append(result)
            
        # Calculate search time
        search_time_ms = int((time.time() - start_time) * 1000)
        
        response = SearchResponse(
            query=request.query,
            results=results,
            total_results=len(results),
            search_time_ms=search_time_ms,
            similarity_threshold_used=request.similarity_threshold,
        )
        
        logger.info(f"Search completed: {len(results)} results in {search_time_ms}ms")
        return response
        
    except Exception as e:
        logger.error(f"Error performing search: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to perform search: {str(e)}"
        )


@router.get("/models", summary="Get model information")
async def get_model_info(app_state: AppState = Depends(get_app_state)):
    """
    Get information about the models being used for embeddings and chat.
    
    Returns:
        dict: Model information and configuration
    """
    try:
        embedding_info = app_state.embedding_service.get_model_info()
        
        return {
            "embedding_model": embedding_info,
            "chat_model": {
                "model": app_state.settings.openai_chat_model,
                "provider": "OpenAI",
                "max_tokens": 4096,  # GPT-4 context length
            },
            "configuration": {
                "embedding_dimension": app_state.settings.embedding_dimension,
                "similarity_threshold": app_state.settings.similarity_threshold,
                "max_chunks_retrieved": app_state.settings.max_chunks_retrieved,
                "batch_size": app_state.settings.batch_size,
            }
        }
        
    except Exception as e:
        logger.error(f"Error getting model info: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get model information: {str(e)}"
        )


@router.get("/content-types", summary="Get available content types")
async def get_content_types():
    """
    Get list of available content types for filtering.
    
    Returns:
        dict: Available content types and their descriptions
    """
    return {
        "content_types": [
            {
                "value": ContentType.DEFINITION.value,
                "description": "Regulatory definitions and terms"
            },
            {
                "value": ContentType.REQUIREMENT.value,
                "description": "Compliance requirements and obligations"
            },
            {
                "value": ContentType.GENERAL.value,
                "description": "General regulatory text"
            },
            {
                "value": ContentType.PENALTY.value,
                "description": "Penalties and enforcement information"
            },
            {
                "value": ContentType.PROCEDURE.value,
                "description": "Procedures and processes"
            },
        ]
    }


@router.get("/sections", summary="Get available sections")
async def get_sections(app_state: AppState = Depends(get_app_state)):
    """
    Get summary of available regulation sections.
    
    Returns:
        dict: Available sections and statistics
    """
    try:
        sections_summary = await app_state.ingestion_service.repository.get_sections_summary()
        return sections_summary
        
    except Exception as e:
        logger.error(f"Error getting sections: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get sections: {str(e)}"
        )

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/services/enhanced_qa_service.py
File type: .py
"""
Enhanced QA Service for 100% Accuracy

This enhanced QA service addresses specific issues identified in testing:
1. Better domain-aware retrieval (Privacy vs Security)
2. Improved system prompts for accurate responses
3. Enhanced context construction for precise citations
4. Better similarity thresholding
"""

import time
import logging
from typing import List, Optional, Tuple, Dict, Any

import openai
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from ..config import Settings
from ..database import DatabaseManager
from ..schemas import AnswerResponse, SourceReference, ContentType
from .embedding_service import EmbeddingService


logger = logging.getLogger(__name__)


class EnhancedQAService:
    """Enhanced QA Service targeting 100% accuracy on HIPAA questions."""
    
    def __init__(
        self,
        db_manager: DatabaseManager,
        embedding_service: EmbeddingService,
        settings: Optional[Settings] = None,
    ) -> None:
        self.db_manager = db_manager
        self.embedding_service = embedding_service
        self.settings = settings or Settings()
        self.repository = db_manager.get_repository()
        
        # OpenAI client
        self.client = openai.AsyncOpenAI(api_key=self.settings.openai_api_key)
        self.chat_model = self.settings.openai_chat_model
        
        # Enhanced retrieval settings
        self.domain_keywords = {
            'privacy': ['privacy', 'protected health information', 'phi', 'uses and disclosures', 
                       'authorization', 'minimum necessary', 'disclosure', 'family members'],
            'security': ['security', 'safeguards', 'access control', 'encryption', 'integrity', 
                        'transmission', 'audit', 'technical safeguards'],
            'penalties': ['penalties', 'civil money penalty', 'violation', 'fine', 'sanctions'],
            'general': ['covered entities', 'business associates', 'applicability', 'definitions'],
            'transactions': ['transactions', 'standards', 'code sets', 'identifiers']
        }
    
    async def answer_question(
        self,
        question: str,
        max_chunks: int = 8,  # Increased for better coverage
        similarity_threshold: float = 0.1,  # Lower threshold for better recall
        content_types: Optional[List[ContentType]] = None,
        sections: Optional[List[str]] = None,
    ) -> AnswerResponse:
        """Answer a question with enhanced accuracy."""
        start_time = time.time()
        
        logger.info(f"üîç Processing enhanced question: {question[:100]}...")
        
        try:
            # Step 1: Detect question domain for better retrieval
            question_domain = self._detect_question_domain(question)
            logger.info(f"üè∑Ô∏è Detected question domain: {question_domain}")
            
            # Step 2: Generate question embedding
            question_embedding = await self.embedding_service.embed_text(question)
            
            # Step 3: Enhanced retrieval with domain awareness
            chunk_results = await self._enhanced_retrieval(
                question=question,
                question_embedding=question_embedding,
                question_domain=question_domain,
                max_chunks=max_chunks,
                similarity_threshold=similarity_threshold,
                content_types=content_types,
                sections=sections,
            )
            
            if not chunk_results:
                logger.warning("No relevant chunks found for question")
                return self._create_no_answer_response(
                    question, start_time, similarity_threshold
                )
                
            logger.info(f"üìã Retrieved {len(chunk_results)} relevant chunks from domain: {question_domain}")
            
            # Step 4: Enhanced context construction
            context = self._build_enhanced_context(chunk_results, question_domain)
            
            # Step 5: Generate answer with enhanced prompts
            answer_text = await self._generate_enhanced_answer(question, context, question_domain)
            
            # Step 6: Build enhanced source references
            sources = self._build_enhanced_source_references(chunk_results)
            
            # Step 7: Calculate processing time and build response
            processing_time_ms = int((time.time() - start_time) * 1000)
            confidence_score = self._calculate_enhanced_confidence(chunk_results, question_domain)
            
            return AnswerResponse(
                question=question,
                answer=answer_text,
                sources=sources,
                confidence_score=confidence_score,
                processing_time_ms=processing_time_ms,
                model_used=self.chat_model,
                chunks_retrieved=len(chunk_results),
                metadata={
                    "question_domain": question_domain,
                    "similarity_threshold": similarity_threshold,
                    "enhancement_version": "1.0"
                }
            )
            
        except Exception as e:
            logger.error(f"‚ùå Error in enhanced QA processing: {e}")
            raise
    
    def _detect_question_domain(self, question: str) -> str:
        """Detect the regulatory domain of the question."""
        question_lower = question.lower()
        domain_scores = {}
        
        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for keyword in keywords if keyword in question_lower)
            domain_scores[domain] = score
        
        # Special domain detection rules
        if any(word in question_lower for word in ['privacy', 'protected health information', 'phi', 'disclosure', 'family']):
            domain_scores['privacy'] = domain_scores.get('privacy', 0) + 5
            
        if any(word in question_lower for word in ['security', 'encryption', 'safeguards', 'access control']):
            domain_scores['security'] = domain_scores.get('security', 0) + 5
            
        if any(word in question_lower for word in ['penalty', 'penalties', 'fine', 'civil money', 'violation']):
            domain_scores['penalties'] = domain_scores.get('penalties', 0) + 5
            
        if any(word in question_lower for word in ['covered entities', 'business associate', 'applicability']):
            domain_scores['general'] = domain_scores.get('general', 0) + 3
        
        # Get domain with highest score
        best_domain = max(domain_scores.items(), key=lambda x: x[1])
        return best_domain[0] if best_domain[1] > 0 else 'general'
    
    async def _enhanced_retrieval(
        self,
        question: str,
        question_embedding: List[float],
        question_domain: str,
        max_chunks: int,
        similarity_threshold: float,
        content_types: Optional[List[ContentType]] = None,
        sections: Optional[List[str]] = None,
    ) -> List[Tuple]:
        """Enhanced retrieval with domain awareness."""
        
        # First, try domain-specific retrieval
        domain_chunks = await self._retrieve_by_domain(
            question_embedding, question_domain, max_chunks // 2, similarity_threshold
        )
        
        # Then, general retrieval for additional context
        general_chunks = await self.repository.similarity_search(
            query_embedding=question_embedding,
            limit=max_chunks,
            similarity_threshold=similarity_threshold,
            content_types=content_types,
            sections=sections,
        )
        
        # Combine and deduplicate
        all_chunks = domain_chunks + general_chunks
        seen_chunk_ids = set()
        unique_chunks = []
        
        for chunk in all_chunks:
            chunk_id = chunk[0]  # Assuming first element is chunk_id
            if chunk_id not in seen_chunk_ids:
                seen_chunk_ids.add(chunk_id)
                unique_chunks.append(chunk)
        
        # Sort by similarity score (descending)
        unique_chunks.sort(key=lambda x: x[-1], reverse=True)  # Last element is similarity
        
        return unique_chunks[:max_chunks]
    
    async def _retrieve_by_domain(
        self, 
        query_embedding: List[float], 
        domain: str, 
        limit: int, 
        threshold: float
    ) -> List[Tuple]:
        """Retrieve chunks filtered by regulation domain."""
        # This would require database schema enhancement to filter by domain
        # For now, fall back to regular retrieval
        return await self.repository.similarity_search(
            query_embedding=query_embedding,
            limit=limit,
            similarity_threshold=threshold
        )
    
    def _build_enhanced_context(self, chunk_results: List[Tuple], question_domain: str) -> str:
        """Build enhanced context with domain awareness."""
        context_parts = []
        
        # Group chunks by domain and section
        domain_chunks = {}
        for i, chunk in enumerate(chunk_results):
            chunk_id, content, section_id, cfr_citation, references_json, key_terms_json, similarity = chunk
            
            # Determine chunk domain (would be better with actual domain metadata)
            if 'privacy' in question_domain and ('164.5' in section_id or 'privacy' in content.lower()):
                chunk_domain = 'privacy'
            elif 'security' in question_domain and ('164.3' in section_id or 'security' in content.lower()):
                chunk_domain = 'security'
            else:
                chunk_domain = 'general'
            
            if chunk_domain not in domain_chunks:
                domain_chunks[chunk_domain] = []
            domain_chunks[chunk_domain].append((i + 1, chunk))
        
        # Build context with domain grouping
        for domain, chunks in domain_chunks.items():
            if chunks:
                context_parts.append(f"=== {domain.upper()} REGULATIONS ===")
                for source_num, chunk in chunks:
                    chunk_id, content, section_id, cfr_citation, references_json, key_terms_json, similarity = chunk
                    context_part = f"[Source {source_num} - {cfr_citation or section_id}]\n{content}"
                    context_parts.append(context_part)
                context_parts.append("")  # Empty line between domains
        
        return "\n".join(context_parts)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError)),
    )
    async def _generate_enhanced_answer(self, question: str, context: str, question_domain: str) -> str:
        """Generate answer with enhanced prompts."""
        
        system_prompt = self._get_enhanced_system_prompt(question_domain)
        user_prompt = self._build_enhanced_user_prompt(context, question, question_domain)
        
        try:
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.0,  # Zero temperature for maximum consistency
                max_tokens=2500,  # Increased for comprehensive answers
                top_p=1.0,
            )
            
            if not response.choices:
                raise ValueError("No response generated by the model")
                
            answer = response.choices[0].message.content
            
            if not answer:
                raise ValueError("Empty response generated by the model")
                
            logger.debug(f"Generated enhanced answer length: {len(answer)} characters")
            return answer.strip()
            
        except openai.OpenAIError as e:
            logger.error(f"OpenAI API error during enhanced answer generation: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error during enhanced answer generation: {e}")
            raise
    
    def _get_enhanced_system_prompt(self, question_domain: str) -> str:
        """Get enhanced system prompt based on question domain."""
        
        base_prompt = """You are a HIPAA expert AI assistant with access to the complete text of the HIPAA regulations (45 CFR Parts 160, 162, and 164). Your role is to provide accurate, precise answers to questions about HIPAA compliance and regulations.

CRITICAL INSTRUCTIONS:
1. Answer questions using ONLY the information provided in the context sources
2. Quote the exact relevant text from the regulations when possible
3. Always cite the specific CFR section number (e.g., "45 CFR ¬ß 164.502") for each factual statement
4. If the answer requires information from multiple sections, cite each one
5. If the provided context doesn't contain sufficient information to answer the question, clearly state that you don't have enough information
6. Do not provide information outside the given sources
7. Use clear, professional language appropriate for legal/regulatory context
8. Structure your answer logically with proper citations throughout

FORMAT FOR CITATIONS:
- Use format: "According to 45 CFR ¬ß [section], [quoted text or paraphrased content]"
- Place citations immediately after the relevant statement
- If quoting directly, use quotation marks around the regulatory text

ANSWER STRUCTURE:
1. Direct answer to the question
2. Supporting quotes from the regulation with proper citations
3. Additional relevant context if helpful
4. Clear indication if information is incomplete"""

        domain_specific_guidance = {
            'privacy': """
PRIVACY RULE SPECIFIC GUIDANCE:
- Focus on Part 164, Subpart E (Privacy of Individually Identifiable Health Information)
- Key sections: 164.500-164.534
- Pay special attention to uses and disclosures (164.502), minimum necessary (164.502(b)), authorizations (164.508), and permitted disclosures (164.510-164.512)
- Distinguish between Privacy Rule (Subpart E) and Security Rule (Subpart C)""",
            
            'security': """
SECURITY RULE SPECIFIC GUIDANCE:
- Focus on Part 164, Subpart C (Security Standards for Electronic Protected Health Information)
- Key sections: 164.302-164.318
- Pay attention to administrative (164.308), physical (164.310), and technical safeguards (164.312)
- Distinguish between required and addressable implementation specifications""",
            
            'penalties': """
PENALTIES SPECIFIC GUIDANCE:
- Focus on Part 160, Subpart D (Civil Money Penalties)
- Key sections: 160.400-160.426
- Pay attention to penalty amounts (160.404), factors considered (160.408), and violation categories
- Include specific dollar amounts and penalty tiers when relevant""",
            
            'general': """
GENERAL PROVISIONS GUIDANCE:
- Focus on Part 160 (General Administrative Requirements)
- Key sections: definitions (160.103), applicability (160.102), covered entities and business associates
- Provide clear distinctions between different entity types and their obligations"""
        }
        
        domain_guidance = domain_specific_guidance.get(question_domain, "")
        
        return base_prompt + "\n" + domain_guidance
    
    def _build_enhanced_user_prompt(self, context: str, question: str, question_domain: str) -> str:
        """Build enhanced user prompt with domain context."""
        
        domain_instructions = {
            'privacy': "Focus on privacy-related provisions and clearly distinguish between Privacy Rule (164.E) and Security Rule (164.C).",
            'security': "Focus on security safeguards and technical requirements for electronic PHI protection.",
            'penalties': "Provide specific penalty amounts and cite the exact penalty calculation provisions.",
            'general': "Provide clear definitions and explain applicability to different entity types."
        }
        
        instruction = domain_instructions.get(question_domain, "Provide a comprehensive answer with exact citations.")
        
        return f"""Based on the following HIPAA regulation excerpts, please answer the user's question with exact citations.

{instruction}

REGULATION CONTEXT:
{context}

USER QUESTION: {question}

Please provide a comprehensive answer with proper CFR citations, focusing on the {question_domain} domain."""
    
    def _build_enhanced_source_references(self, chunk_results: List[Tuple]) -> List[SourceReference]:
        """Build enhanced source references."""
        sources = []
        
        for i, chunk in enumerate(chunk_results):
            chunk_id, content, section_id, cfr_citation, references_json, key_terms_json, similarity = chunk
            
            sources.append(SourceReference(
                source_id=f"source_{i + 1}",
                title=cfr_citation or f"45 CFR ¬ß {section_id}",
                section_reference=cfr_citation or f"45 CFR ¬ß {section_id}",
                content_preview=content[:200] + "..." if len(content) > 200 else content,
                similarity_score=float(similarity),
                metadata={
                    "chunk_id": chunk_id,
                    "section_id": section_id,
                    "full_content_length": len(content)
                }
            ))
        
        return sources
    
    def _calculate_enhanced_confidence(self, chunk_results: List[Tuple], question_domain: str) -> float:
        """Calculate enhanced confidence score."""
        if not chunk_results:
            return 0.0
        
        # Base confidence from similarity scores
        similarities = [float(chunk[-1]) for chunk in chunk_results]  # Last element is similarity
        avg_similarity = sum(similarities) / len(similarities)
        
        # Boost confidence for domain-specific matches
        domain_boost = 0.0
        for chunk in chunk_results:
            content = chunk[1].lower()
            if question_domain == 'privacy' and any(word in content for word in ['privacy', 'protected health information', 'disclosure']):
                domain_boost += 0.1
            elif question_domain == 'security' and any(word in content for word in ['security', 'safeguards', 'encryption']):
                domain_boost += 0.1
            elif question_domain == 'penalties' and any(word in content for word in ['penalty', 'civil money', 'violation']):
                domain_boost += 0.1
        
        # Confidence calculation
        confidence = min(avg_similarity + domain_boost, 1.0)
        
        return round(confidence, 3)
    
    def _create_no_answer_response(
        self,
        question: str,
        start_time: float,
        similarity_threshold: float
    ) -> AnswerResponse:
        """Create response when no relevant chunks are found."""
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        no_answer_text = (
            "I apologize, but I couldn't find relevant information in the HIPAA "
            "regulations to answer your question. This could mean:\n\n"
            "1. The question is outside the scope of HIPAA regulations (Parts 160, 162, 164)\n"
            "2. The question might need to be rephrased more specifically\n"
            "3. The information might be in a section not well-matched by the search\n\n"
            "Please try rephrasing your question or asking about a specific HIPAA topic "
            "like privacy rules, security standards, breach notification, or covered entities."
        )
        
        return AnswerResponse(
            question=question,
            answer=no_answer_text,
            sources=[],
            confidence_score=0.0,
            processing_time_ms=processing_time_ms,
            model_used=self.chat_model,
            chunks_retrieved=0,
            metadata={
                "reason": "no_relevant_chunks",
                "similarity_threshold": similarity_threshold,
                "enhancement_version": "1.0"
            }
        )

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/services/__init__.py
File type: .py
"""Services module for HIPAA QA System."""

from .embedding_service import EmbeddingService
from .ingestion_service import IngestionService
from .qa_service import QAService

__all__ = [
    "EmbeddingService",
    "IngestionService", 
    "QAService",
]

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/services/embedding_service.py
File type: .py
"""OpenAI embedding service for text vectorization."""

import asyncio
from typing import List, Optional

import openai
from loguru import logger
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from ..config import Settings, get_settings


class EmbeddingService:
    """Service for generating embeddings using OpenAI."""
    
    def __init__(self, settings: Optional[Settings] = None) -> None:
        """Initialize embedding service."""
        self.settings = settings or get_settings()
        
        # Configure OpenAI client
        self.client = openai.AsyncOpenAI(
            api_key=self.settings.openai_api_key,
            timeout=self.settings.openai_timeout,
            max_retries=self.settings.openai_max_retries,
        )
        
        self.model = self.settings.openai_embedding_model
        logger.info(f"Initialized embedding service with model: {self.model}")
        
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError)),
    )
    async def embed_text(self, text: str) -> List[float]:
        """
        Generate embedding for a single text.
        
        Args:
            text: Text to embed
            
        Returns:
            List of embedding values
            
        Raises:
            openai.OpenAIError: If API call fails after retries
        """
        if not text.strip():
            raise ValueError("Text cannot be empty")
            
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=text.strip(),
                encoding_format="float"
            )
            
            embedding = response.data[0].embedding
            
            # Validate embedding dimension
            expected_dim = self.settings.embedding_dimension
            if len(embedding) != expected_dim:
                raise ValueError(
                    f"Unexpected embedding dimension: got {len(embedding)}, "
                    f"expected {expected_dim}"
                )
                
            logger.debug(f"Generated embedding for text (length: {len(text)})")
            return embedding
            
        except openai.OpenAIError as e:
            logger.error(f"OpenAI API error during embedding: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error during embedding: {e}")
            raise
            
    async def embed_texts_batch(
        self, 
        texts: List[str],
        batch_size: int = 100
    ) -> List[List[float]]:
        """
        Generate embeddings for multiple texts in batches.
        
        Args:
            texts: List of texts to embed
            batch_size: Number of texts to process per API call
            
        Returns:
            List of embeddings corresponding to input texts
            
        Raises:
            openai.OpenAIError: If API calls fail after retries
        """
        if not texts:
            return []
            
        # Filter out empty texts and track indices
        valid_texts = []
        valid_indices = []
        for i, text in enumerate(texts):
            if text.strip():
                valid_texts.append(text.strip())
                valid_indices.append(i)
            else:
                logger.warning(f"Skipping empty text at index {i}")
                
        if not valid_texts:
            raise ValueError("No valid texts to embed")
            
        logger.info(f"Embedding {len(valid_texts)} texts in batches of {batch_size}")
        
        all_embeddings = []
        
        for i in range(0, len(valid_texts), batch_size):
            batch = valid_texts[i:i + batch_size]
            
            try:
                embeddings = await self._embed_batch(batch)
                all_embeddings.extend(embeddings)
                
                logger.info(
                    f"Processed batch {i//batch_size + 1}/"
                    f"{(len(valid_texts)-1)//batch_size + 1}"
                )
                
                # Rate limiting: small delay between batches
                if i + batch_size < len(valid_texts):
                    await asyncio.sleep(0.1)
                    
            except Exception as e:
                logger.error(f"Failed to process batch {i//batch_size + 1}: {e}")
                raise
                
        # Reconstruct full results list with None for empty texts
        results = [None] * len(texts)  # type: ignore
        for i, embedding in enumerate(all_embeddings):
            original_index = valid_indices[i]
            results[original_index] = embedding
            
        logger.info(f"Successfully embedded {len(all_embeddings)} texts")
        return results  # type: ignore
        
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError)),
    )
    async def _embed_batch(self, texts: List[str]) -> List[List[float]]:
        """Internal method to embed a batch of texts."""
        try:
            response = await self.client.embeddings.create(
                model=self.model,
                input=texts,
                encoding_format="float"
            )
            
            embeddings = [data.embedding for data in response.data]
            
            # Validate all embeddings have correct dimension
            expected_dim = self.settings.embedding_dimension
            for i, embedding in enumerate(embeddings):
                if len(embedding) != expected_dim:
                    raise ValueError(
                        f"Unexpected embedding dimension for text {i}: "
                        f"got {len(embedding)}, expected {expected_dim}"
                    )
                    
            return embeddings
            
        except openai.OpenAIError as e:
            logger.error(f"OpenAI API error during batch embedding: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error during batch embedding: {e}")
            raise
            
    async def validate_api_access(self) -> bool:
        """
        Validate that the OpenAI API is accessible with current credentials.
        
        Returns:
            True if API is accessible, False otherwise
        """
        try:
            # Test with a simple embedding request
            test_response = await self.client.embeddings.create(
                model=self.model,
                input="test",
                encoding_format="float"
            )
            
            if test_response.data and len(test_response.data) > 0:
                embedding = test_response.data[0].embedding
                expected_dim = self.settings.embedding_dimension
                
                if len(embedding) == expected_dim:
                    logger.info("OpenAI embedding API validation successful")
                    return True
                else:
                    logger.error(
                        f"API validation failed: unexpected embedding dimension "
                        f"{len(embedding)}, expected {expected_dim}"
                    )
                    return False
            else:
                logger.error("API validation failed: no embedding data returned")
                return False
                
        except Exception as e:
            logger.error(f"OpenAI API validation failed: {e}")
            return False
            
    def get_model_info(self) -> dict:
        """Get information about the current embedding model."""
        return {
            "model": self.model,
            "dimension": self.settings.embedding_dimension,
            "max_tokens": 8191,  # text-embedding-3-large limit
            "cost_per_1k_tokens": 0.00013,  # As of latest pricing
        }

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/services/qa_service.py
File type: .py
"""Question-answering service using RAG with OpenAI."""

import time
from typing import List, Optional, Tuple

import openai
from loguru import logger
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from ..config import Settings, get_settings
from ..database import ChunkRepository, DatabaseManager
from ..schemas import AnswerResponse, ContentType, SourceReference
from .embedding_service import EmbeddingService


class QAService:
    """Service for question-answering using RAG pipeline."""
    
    def __init__(
        self,
        db_manager: DatabaseManager,
        embedding_service: EmbeddingService,
        settings: Optional[Settings] = None,
    ) -> None:
        """Initialize QA service."""
        self.db_manager = db_manager
        self.embedding_service = embedding_service
        self.repository = ChunkRepository(db_manager)
        self.settings = settings or get_settings()
        
        # Configure OpenAI client
        self.client = openai.AsyncOpenAI(
            api_key=self.settings.openai_api_key,
            timeout=self.settings.openai_timeout,
            max_retries=self.settings.openai_max_retries,
        )
        
        self.chat_model = self.settings.openai_chat_model
        logger.info(f"Initialized QA service with model: {self.chat_model}")
        
    async def answer_question(
        self,
        question: str,
        max_chunks: int = 5,
        similarity_threshold: float = 0.7,
        content_types: Optional[List[ContentType]] = None,
        sections: Optional[List[str]] = None,
    ) -> AnswerResponse:
        """
        Answer a question using RAG pipeline.
        
        Args:
            question: User's question about HIPAA
            max_chunks: Maximum number of context chunks to retrieve
            similarity_threshold: Minimum similarity for relevant chunks
            content_types: Optional filter by content types
            sections: Optional filter by specific sections
            
        Returns:
            AnswerResponse with generated answer and source references
        """
        start_time = time.time()
        
        logger.info(f"Processing question: {question[:100]}...")
        
        try:
            # Step 1: Generate question embedding
            question_embedding = await self.embedding_service.embed_text(question)
            
            # Step 2: Retrieve relevant chunks
            chunk_results = await self.repository.similarity_search(
                query_embedding=question_embedding,
                limit=max_chunks,
                similarity_threshold=similarity_threshold,
                content_types=content_types,
                sections=sections,
            )
            
            if not chunk_results:
                logger.warning("No relevant chunks found for question")
                return self._create_no_answer_response(
                    question, start_time, similarity_threshold
                )
                
            logger.info(f"Retrieved {len(chunk_results)} relevant chunks")
            
            # Step 3: Construct context and generate answer
            context = self._build_context(chunk_results)
            answer_text = await self._generate_answer(question, context)
            
            # Step 4: Build source references
            sources = self._build_source_references(chunk_results)
            
            # Step 5: Calculate processing time and build response
            processing_time_ms = int((time.time() - start_time) * 1000)
            
            # Calculate confidence score based on similarity scores
            confidence_score = self._calculate_confidence(chunk_results)
            
            return AnswerResponse(
                question=question,
                answer=answer_text,
                sources=sources,
                confidence_score=confidence_score,
                processing_time_ms=processing_time_ms,
                model_used=self.chat_model,
                chunks_retrieved=len(chunk_results),
                metadata={
                    "similarity_threshold": similarity_threshold,
                    "embedding_model": self.embedding_service.model,
                    "max_chunks_requested": max_chunks,
                }
            )
            
        except Exception as e:
            logger.error(f"Error processing question: {e}")
            processing_time_ms = int((time.time() - start_time) * 1000)
            
            return AnswerResponse(
                question=question,
                answer=f"I apologize, but I encountered an error processing your question: {str(e)}",
                sources=[],
                confidence_score=0.0,
                processing_time_ms=processing_time_ms,
                model_used=self.chat_model,
                chunks_retrieved=0,
                metadata={"error": str(e)}
            )
            
    def _build_context(self, chunk_results: List[Tuple]) -> str:
        """Build context string from retrieved chunks."""
        context_parts = []
        
        for i, (chunk, similarity_score) in enumerate(chunk_results, 1):
            # Format each chunk with clear section identification
            section_ref = chunk.cfr_citation or chunk.full_reference
            context_part = (
                f"[Source {i}: {section_ref}]\n"
                f"{chunk.content}\n"
            )
            context_parts.append(context_part)
            
        return "\n".join(context_parts)
        
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError)),
    )
    async def _generate_answer(self, question: str, context: str) -> str:
        """Generate answer using OpenAI chat completion."""
        
        system_prompt = self._get_system_prompt()
        user_prompt = self._build_user_prompt(context, question)
        
        try:
            response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,  # Low temperature for consistent, factual responses
                max_tokens=2000,  # Reasonable limit for responses
                top_p=0.9,
            )
            
            if not response.choices:
                raise ValueError("No response generated by the model")
                
            answer = response.choices[0].message.content
            
            if not answer:
                raise ValueError("Empty response generated by the model")
                
            logger.debug(f"Generated answer length: {len(answer)} characters")
            return answer.strip()
            
        except openai.OpenAIError as e:
            logger.error(f"OpenAI API error during answer generation: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error during answer generation: {e}")
            raise
            
    def _get_system_prompt(self) -> str:
        """Get the system prompt for the AI assistant."""
        return """You are a HIPAA expert AI assistant with access to the complete text of the HIPAA regulations (45 CFR Parts 160, 162, and 164). Your role is to provide accurate, precise answers to questions about HIPAA compliance and regulations.

CRITICAL INSTRUCTIONS:
1. Answer questions using ONLY the information provided in the context sources
2. Quote the exact relevant text from the regulations when possible
3. Always cite the specific CFR section number (e.g., "45 CFR ¬ß 164.502") for each factual statement
4. If the answer requires information from multiple sections, cite each one
5. If the provided context doesn't contain sufficient information to answer the question, clearly state that you don't have enough information
6. Do not provide information outside the given sources
7. Use clear, professional language appropriate for legal/regulatory context
8. Structure your answer logically with proper citations throughout

FORMAT FOR CITATIONS:
- Use format: "According to 45 CFR ¬ß [section], [quoted text or paraphrased content]"
- Place citations immediately after the relevant statement
- If quoting directly, use quotation marks around the regulatory text

ANSWER STRUCTURE:
1. Direct answer to the question
2. Supporting quotes from the regulation with proper citations
3. Additional relevant context if helpful
4. Clear indication if information is incomplete"""

    def _build_user_prompt(self, context: str, question: str) -> str:
        """Build the user prompt with context and question."""
        return f"""Based on the following HIPAA regulation excerpts, please answer the user's question with exact citations.

REGULATION CONTEXT:
{context}

USER QUESTION: {question}

Please provide a comprehensive answer with proper CFR citations."""
        
    def _build_source_references(self, chunk_results: List[Tuple]) -> List[SourceReference]:
        """Build source reference objects from chunk results."""
        sources = []
        
        for chunk, similarity_score in chunk_results:
            # Create content excerpt (first 200 chars + ellipsis if longer)
            content_excerpt = chunk.content
            if len(content_excerpt) > 200:
                content_excerpt = content_excerpt[:200] + "..."
                
            source = SourceReference(
                section_id=chunk.section_id,
                cfr_citation=chunk.cfr_citation or chunk.full_reference,
                section_title=chunk.section_title,
                content_excerpt=content_excerpt,
                similarity_score=round(similarity_score, 3),
                chunk_id=chunk.chunk_id,
            )
            sources.append(source)
            
        return sources
        
    def _calculate_confidence(self, chunk_results: List[Tuple]) -> float:
        """Calculate confidence score based on similarity scores."""
        if not chunk_results:
            return 0.0
            
        # Use the highest similarity score as primary confidence indicator
        max_similarity = max(similarity for _, similarity in chunk_results)
        
        # Apply additional factors
        num_chunks = len(chunk_results)
        
        # Base confidence on max similarity
        confidence = max_similarity
        
        # Boost confidence if we have multiple relevant chunks
        if num_chunks >= 3:
            confidence = min(confidence * 1.1, 1.0)
        elif num_chunks >= 2:
            confidence = min(confidence * 1.05, 1.0)
            
        return round(confidence, 3)
        
    def _create_no_answer_response(
        self,
        question: str,
        start_time: float,
        similarity_threshold: float
    ) -> AnswerResponse:
        """Create response when no relevant chunks are found."""
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        no_answer_text = (
            "I apologize, but I couldn't find relevant information in the HIPAA "
            "regulations to answer your question. This could mean:\n\n"
            "1. The question is outside the scope of HIPAA regulations (Parts 160, 162, 164)\n"
            "2. The question might need to be rephrased more specifically\n"
            "3. The information might be in a section not well-matched by the search\n\n"
            "Please try rephrasing your question or asking about a specific HIPAA topic "
            "like privacy rules, security standards, breach notification, or covered entities."
        )
        
        return AnswerResponse(
            question=question,
            answer=no_answer_text,
            sources=[],
            confidence_score=0.0,
            processing_time_ms=processing_time_ms,
            model_used=self.chat_model,
            chunks_retrieved=0,
            metadata={
                "reason": "no_relevant_chunks",
                "similarity_threshold": similarity_threshold,
            }
        )
        
    async def validate_chat_api_access(self) -> bool:
        """
        Validate that the OpenAI Chat API is accessible.
        
        Returns:
            True if API is accessible, False otherwise
        """
        try:
            test_response = await self.client.chat.completions.create(
                model=self.chat_model,
                messages=[{"role": "user", "content": "Hello"}],
                max_tokens=10,
                temperature=0.1,
            )
            
            if test_response.choices and test_response.choices[0].message.content:
                logger.info("OpenAI Chat API validation successful")
                return True
            else:
                logger.error("Chat API validation failed: no response content")
                return False
                
        except Exception as e:
            logger.error(f"OpenAI Chat API validation failed: {e}")
            return False

--------------------------------------------------
File End
--------------------------------------------------


src/hipaa_qa/services/ingestion_service.py
File type: .py
"""Service for ingesting and processing HIPAA document chunks."""

import json
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

from loguru import logger

from ..database import ChunkRepository, DatabaseManager
from ..schemas import ChunkMetadata, ComplianceLevel, ContentType, DocumentChunk, SectionType
from .embedding_service import EmbeddingService


class IngestionService:
    """Service for ingesting document chunks into the database."""
    
    def __init__(
        self,
        db_manager: DatabaseManager,
        embedding_service: EmbeddingService,
    ) -> None:
        """Initialize ingestion service."""
        self.db_manager = db_manager
        self.embedding_service = embedding_service
        self.repository = ChunkRepository(db_manager)
        
    async def ingest_from_json(
        self,
        json_file_path: str,
        batch_size: int = 100,
        overwrite_existing: bool = False,
        generate_embeddings: bool = True,
    ) -> Tuple[int, int, List[str]]:
        """
        Ingest document chunks from JSON file.
        
        Args:
            json_file_path: Path to the chunks JSON file
            batch_size: Number of chunks to process per batch
            overwrite_existing: Whether to overwrite existing data
            generate_embeddings: Whether to generate embeddings
            
        Returns:
            Tuple of (success_count, error_count, error_messages)
        """
        logger.info(f"Starting ingestion from {json_file_path}")
        
        # Validate file exists
        json_path = Path(json_file_path)
        if not json_path.exists():
            raise FileNotFoundError(f"Chunks file not found: {json_file_path}")
            
        # Clear existing data if requested
        if overwrite_existing:
            deleted_count = await self.repository.delete_all_chunks()
            logger.info(f"Cleared {deleted_count} existing chunks")
            
        # Load and parse JSON data
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                raw_chunks = json.load(f)
        except Exception as e:
            raise ValueError(f"Failed to load JSON file: {e}")
            
        logger.info(f"Loaded {len(raw_chunks)} chunks from JSON")
        
        # Convert to DocumentChunk objects
        chunks = []
        conversion_errors = []
        
        for i, raw_chunk in enumerate(raw_chunks):
            try:
                chunk = self._convert_raw_chunk(raw_chunk)
                chunks.append(chunk)
            except Exception as e:
                error_msg = f"Failed to convert chunk {i}: {e}"
                conversion_errors.append(error_msg)
                logger.warning(error_msg)
                
        logger.info(f"Converted {len(chunks)} chunks, {len(conversion_errors)} conversion errors")
        
        # Generate embeddings if requested
        if generate_embeddings:
            logger.info("Generating embeddings for chunks...")
            await self._add_embeddings_to_chunks(chunks, batch_size)
            
        # Insert chunks into database
        success_count, error_count = await self.repository.bulk_insert_chunks(
            chunks, batch_size
        )
        
        all_errors = conversion_errors
        if error_count > 0:
            all_errors.append(f"{error_count} chunks failed database insertion")
            
        logger.info(
            f"Ingestion completed: {success_count} successful, "
            f"{error_count + len(conversion_errors)} errors"
        )
        
        return success_count, error_count + len(conversion_errors), all_errors
        
    def _convert_raw_chunk(self, raw_chunk: dict) -> DocumentChunk:
        """Convert raw JSON chunk data to DocumentChunk object."""
        try:
            # Extract metadata
            raw_metadata = raw_chunk.get("metadata", {})
            
            # Map section type
            section_type_map = {
                "part": SectionType.PART,
                "subpart": SectionType.SUBPART, 
                "section": SectionType.SECTION,
                "subsection": SectionType.SUBSECTION,
                "paragraph": SectionType.PARAGRAPH,
            }
            section_type = section_type_map.get(
                raw_metadata.get("section_type", "section").lower(),
                SectionType.SECTION
            )
            
            # Map content type
            content_type_map = {
                "definition": ContentType.DEFINITION,
                "requirement": ContentType.REQUIREMENT,
                "general": ContentType.GENERAL,
                "penalty": ContentType.PENALTY,
                "procedure": ContentType.PROCEDURE,
            }
            content_type = content_type_map.get(
                raw_metadata.get("content_type", "general").lower(),
                ContentType.GENERAL
            )
            
            # Map compliance level
            compliance_level_map = {
                "mandatory": ComplianceLevel.MANDATORY,
                "required": ComplianceLevel.REQUIRED,
                "permitted": ComplianceLevel.PERMITTED,
                "prohibited": ComplianceLevel.PROHIBITED,
                "informational": ComplianceLevel.INFORMATIONAL,
            }
            compliance_level = compliance_level_map.get(
                raw_metadata.get("compliance_level", "informational").lower(),
                ComplianceLevel.INFORMATIONAL
            )
            
            # Create metadata object
            metadata = ChunkMetadata(
                section_id=raw_metadata.get("section_id", "unknown"),
                section_type=section_type,
                section_title=raw_metadata.get("section_title", ""),
                full_reference=raw_metadata.get("full_reference", ""),
                cfr_citation=raw_metadata.get("cfr_citation"),
                parent_section=raw_metadata.get("parent_section"),
                hierarchy_level=raw_metadata.get("hierarchy_level", 1),
                chunk_index=raw_metadata.get("chunk_index", 0),
                total_chunks=raw_metadata.get("total_chunks", 1),
                chunk_size=raw_metadata.get("chunk_size", 0),
                word_count=raw_metadata.get("word_count", 0),
                contains_definitions=raw_metadata.get("contains_definitions", False),
                contains_penalties=raw_metadata.get("contains_penalties", False),
                contains_requirements=raw_metadata.get("contains_requirements", False),
                references=raw_metadata.get("references", []),
                key_terms=raw_metadata.get("key_terms", []),
                content_type=content_type,
                compliance_level=compliance_level,
            )
            
            # Create DocumentChunk
            chunk = DocumentChunk(
                chunk_id=raw_chunk.get("chunk_id", 0),
                content=raw_chunk.get("content", ""),
                metadata=metadata,
                embedding=None,  # Will be generated later if requested
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
            )
            
            return chunk
            
        except Exception as e:
            raise ValueError(f"Failed to convert raw chunk: {e}")
            
    async def _add_embeddings_to_chunks(
        self,
        chunks: List[DocumentChunk],
        batch_size: int = 100
    ) -> None:
        """Generate embeddings for chunks."""
        logger.info(f"Generating embeddings for {len(chunks)} chunks...")
        
        # Extract content texts
        texts = [chunk.content for chunk in chunks]
        
        # Generate embeddings in batches
        try:
            embeddings = await self.embedding_service.embed_texts_batch(
                texts, batch_size
            )
            
            # Assign embeddings to chunks
            embedded_count = 0
            for i, embedding in enumerate(embeddings):
                if embedding is not None:
                    chunks[i].embedding = embedding
                    embedded_count += 1
                else:
                    logger.warning(f"No embedding generated for chunk {i}")
                    
            logger.info(f"Generated embeddings for {embedded_count}/{len(chunks)} chunks")
            
        except Exception as e:
            logger.error(f"Failed to generate embeddings: {e}")
            raise
            
    async def get_ingestion_status(self) -> dict:
        """Get status of current ingestion."""
        total_chunks = await self.repository.count_chunks()
        embedded_chunks = await self.repository.count_chunks_with_embeddings()
        sections_summary = await self.repository.get_sections_summary()
        
        return {
            "total_chunks": total_chunks,
            "embedded_chunks": embedded_chunks,
            "embedding_coverage": embedded_chunks / total_chunks if total_chunks > 0 else 0,
            "sections_summary": sections_summary,
        }
        
    async def recompute_embeddings(
        self,
        batch_size: int = 100,
        section_filter: Optional[str] = None
    ) -> Tuple[int, int]:
        """
        Recompute embeddings for existing chunks.
        
        Args:
            batch_size: Batch size for processing
            section_filter: Optional section ID to filter chunks
            
        Returns:
            Tuple of (success_count, error_count)
        """
        logger.info("Starting embedding recomputation...")
        
        # Get chunks to recompute
        if section_filter:
            chunks_data = await self.repository.get_chunks_by_section(section_filter)
        else:
            # For now, we'll implement a simple approach
            # In a real implementation, you'd want pagination for large datasets
            logger.warning("Recomputing all embeddings - this may take a while")
            # This would need a method to get all chunks in batches
            raise NotImplementedError("Full recomputation not implemented yet")
            
        if not chunks_data:
            logger.info("No chunks found for recomputation")
            return 0, 0
            
        # Convert to DocumentChunk objects and generate embeddings
        chunks = []
        for chunk_data in chunks_data:
            # Convert database model back to schema
            # This is a simplified conversion - you might need more complete mapping
            metadata = ChunkMetadata(
                section_id=chunk_data.section_id,
                section_type=SectionType(chunk_data.section_type),
                section_title=chunk_data.section_title,
                full_reference=chunk_data.full_reference,
                cfr_citation=chunk_data.cfr_citation,
                parent_section=chunk_data.parent_section,
                hierarchy_level=chunk_data.hierarchy_level,
                chunk_index=chunk_data.chunk_index,
                total_chunks=chunk_data.total_chunks,
                chunk_size=chunk_data.chunk_size,
                word_count=chunk_data.word_count,
                contains_definitions=chunk_data.contains_definitions,
                contains_penalties=chunk_data.contains_penalties,
                contains_requirements=chunk_data.contains_requirements,
                references=chunk_data.references or [],
                key_terms=chunk_data.key_terms or [],
                content_type=ContentType(chunk_data.content_type),
                compliance_level=ComplianceLevel(chunk_data.compliance_level),
            )
            
            chunk = DocumentChunk(
                chunk_id=chunk_data.chunk_id,
                content=chunk_data.content,
                metadata=metadata,
                embedding=None,
                created_at=chunk_data.created_at,
                updated_at=datetime.utcnow(),
            )
            chunks.append(chunk)
            
        # Generate new embeddings
        await self._add_embeddings_to_chunks(chunks, batch_size)
        
        # Update in database
        success_count, error_count = await self.repository.bulk_insert_chunks(
            chunks, batch_size
        )
        
        logger.info(f"Embedding recomputation completed: {success_count} success, {error_count} errors")
        return success_count, error_count

--------------------------------------------------
File End
--------------------------------------------------
